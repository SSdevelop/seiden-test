{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45eeed88-e164-4b61-bc12-e7a3d4ad6a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, GroundingDinoForObjectDetection\n",
    "import cv2\n",
    "import time\n",
    "import torch.cuda as cuda\n",
    "from PIL import Image\n",
    "model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96ea2cb5-a0aa-4320-a898-113df89e11b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGroundingDinoForObjectDetection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/seiden/lib/python3.10/site-packages/transformers/modeling_utils.py:3164\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3160\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3161\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3162\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3163\u001b[0m         )\n\u001b[0;32m-> 3164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/seiden/lib/python3.10/site-packages/torch/nn/modules/module.py:907\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    904\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 907\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/seiden/lib/python3.10/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/seiden/lib/python3.10/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 578 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/seiden/lib/python3.10/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/seiden/lib/python3.10/site-packages/torch/nn/modules/module.py:601\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 601\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.conda/envs/seiden/lib/python3.10/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    904\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 905\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/seiden/lib/python3.10/site-packages/torch/cuda/__init__.py:216\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    220\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = GroundingDinoForObjectDetection.from_pretrained(model_id).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd5553f1-ec68-4b29-9c66-3ea82a41f3a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GroundingDinoForObjectDetection(\n",
       "  (model): GroundingDinoModel(\n",
       "    (backbone): GroundingDinoConvModel(\n",
       "      (conv_encoder): GroundingDinoConvEncoder(\n",
       "        (model): SwinBackbone(\n",
       "          (embeddings): SwinEmbeddings(\n",
       "            (patch_embeddings): SwinPatchEmbeddings(\n",
       "              (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "            )\n",
       "            (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (encoder): SwinEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0): SwinStage(\n",
       "                (blocks): ModuleList(\n",
       "                  (0): SwinLayer(\n",
       "                    (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                    (attention): SwinAttention(\n",
       "                      (self): SwinSelfAttention(\n",
       "                        (query): Linear(in_features=96, out_features=96, bias=True)\n",
       "                        (key): Linear(in_features=96, out_features=96, bias=True)\n",
       "                        (value): Linear(in_features=96, out_features=96, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (output): SwinSelfOutput(\n",
       "                        (dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): Identity()\n",
       "                    (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                    (intermediate): SwinIntermediate(\n",
       "                      (dense): Linear(in_features=96, out_features=384, bias=True)\n",
       "                      (intermediate_act_fn): GELUActivation()\n",
       "                    )\n",
       "                    (output): SwinOutput(\n",
       "                      (dense): Linear(in_features=384, out_features=96, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (1): SwinLayer(\n",
       "                    (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                    (attention): SwinAttention(\n",
       "                      (self): SwinSelfAttention(\n",
       "                        (query): Linear(in_features=96, out_features=96, bias=True)\n",
       "                        (key): Linear(in_features=96, out_features=96, bias=True)\n",
       "                        (value): Linear(in_features=96, out_features=96, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (output): SwinSelfOutput(\n",
       "                        (dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): SwinDropPath(p=0.00909090880304575)\n",
       "                    (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                    (intermediate): SwinIntermediate(\n",
       "                      (dense): Linear(in_features=96, out_features=384, bias=True)\n",
       "                      (intermediate_act_fn): GELUActivation()\n",
       "                    )\n",
       "                    (output): SwinOutput(\n",
       "                      (dense): Linear(in_features=384, out_features=96, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (downsample): SwinPatchMerging(\n",
       "                  (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "                  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "              )\n",
       "              (1): SwinStage(\n",
       "                (blocks): ModuleList(\n",
       "                  (0): SwinLayer(\n",
       "                    (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                    (attention): SwinAttention(\n",
       "                      (self): SwinSelfAttention(\n",
       "                        (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "                        (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "                        (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (output): SwinSelfOutput(\n",
       "                        (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): SwinDropPath(p=0.0181818176060915)\n",
       "                    (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                    (intermediate): SwinIntermediate(\n",
       "                      (dense): Linear(in_features=192, out_features=768, bias=True)\n",
       "                      (intermediate_act_fn): GELUActivation()\n",
       "                    )\n",
       "                    (output): SwinOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=192, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (1): SwinLayer(\n",
       "                    (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                    (attention): SwinAttention(\n",
       "                      (self): SwinSelfAttention(\n",
       "                        (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "                        (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "                        (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (output): SwinSelfOutput(\n",
       "                        (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): SwinDropPath(p=0.027272727340459824)\n",
       "                    (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                    (intermediate): SwinIntermediate(\n",
       "                      (dense): Linear(in_features=192, out_features=768, bias=True)\n",
       "                      (intermediate_act_fn): GELUActivation()\n",
       "                    )\n",
       "                    (output): SwinOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=192, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (downsample): SwinPatchMerging(\n",
       "                  (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "                  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "              )\n",
       "              (2): SwinStage(\n",
       "                (blocks): ModuleList(\n",
       "                  (0): SwinLayer(\n",
       "                    (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                    (attention): SwinAttention(\n",
       "                      (self): SwinSelfAttention(\n",
       "                        (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                        (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                        (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (output): SwinSelfOutput(\n",
       "                        (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): SwinDropPath(p=0.036363635212183)\n",
       "                    (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                    (intermediate): SwinIntermediate(\n",
       "                      (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                      (intermediate_act_fn): GELUActivation()\n",
       "                    )\n",
       "                    (output): SwinOutput(\n",
       "                      (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (1): SwinLayer(\n",
       "                    (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                    (attention): SwinAttention(\n",
       "                      (self): SwinSelfAttention(\n",
       "                        (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                        (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                        (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (output): SwinSelfOutput(\n",
       "                        (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): SwinDropPath(p=0.045454543083906174)\n",
       "                    (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                    (intermediate): SwinIntermediate(\n",
       "                      (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                      (intermediate_act_fn): GELUActivation()\n",
       "                    )\n",
       "                    (output): SwinOutput(\n",
       "                      (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (2): SwinLayer(\n",
       "                    (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                    (attention): SwinAttention(\n",
       "                      (self): SwinSelfAttention(\n",
       "                        (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                        (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                        (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (output): SwinSelfOutput(\n",
       "                        (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): SwinDropPath(p=0.054545458406209946)\n",
       "                    (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                    (intermediate): SwinIntermediate(\n",
       "                      (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                      (intermediate_act_fn): GELUActivation()\n",
       "                    )\n",
       "                    (output): SwinOutput(\n",
       "                      (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (3): SwinLayer(\n",
       "                    (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                    (attention): SwinAttention(\n",
       "                      (self): SwinSelfAttention(\n",
       "                        (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                        (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                        (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (output): SwinSelfOutput(\n",
       "                        (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): SwinDropPath(p=0.06363636255264282)\n",
       "                    (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                    (intermediate): SwinIntermediate(\n",
       "                      (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                      (intermediate_act_fn): GELUActivation()\n",
       "                    )\n",
       "                    (output): SwinOutput(\n",
       "                      (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (4): SwinLayer(\n",
       "                    (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                    (attention): SwinAttention(\n",
       "                      (self): SwinSelfAttention(\n",
       "                        (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                        (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                        (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (output): SwinSelfOutput(\n",
       "                        (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): SwinDropPath(p=0.0727272778749466)\n",
       "                    (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                    (intermediate): SwinIntermediate(\n",
       "                      (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                      (intermediate_act_fn): GELUActivation()\n",
       "                    )\n",
       "                    (output): SwinOutput(\n",
       "                      (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (5): SwinLayer(\n",
       "                    (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                    (attention): SwinAttention(\n",
       "                      (self): SwinSelfAttention(\n",
       "                        (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                        (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                        (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (output): SwinSelfOutput(\n",
       "                        (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): SwinDropPath(p=0.08181818574666977)\n",
       "                    (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                    (intermediate): SwinIntermediate(\n",
       "                      (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                      (intermediate_act_fn): GELUActivation()\n",
       "                    )\n",
       "                    (output): SwinOutput(\n",
       "                      (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (downsample): SwinPatchMerging(\n",
       "                  (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "                  (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "              )\n",
       "              (3): SwinStage(\n",
       "                (blocks): ModuleList(\n",
       "                  (0): SwinLayer(\n",
       "                    (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                    (attention): SwinAttention(\n",
       "                      (self): SwinSelfAttention(\n",
       "                        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (output): SwinSelfOutput(\n",
       "                        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): SwinDropPath(p=0.09090909361839294)\n",
       "                    (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                    (intermediate): SwinIntermediate(\n",
       "                      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                      (intermediate_act_fn): GELUActivation()\n",
       "                    )\n",
       "                    (output): SwinOutput(\n",
       "                      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (1): SwinLayer(\n",
       "                    (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                    (attention): SwinAttention(\n",
       "                      (self): SwinSelfAttention(\n",
       "                        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (output): SwinSelfOutput(\n",
       "                        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): SwinDropPath(p=0.10000000149011612)\n",
       "                    (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                    (intermediate): SwinIntermediate(\n",
       "                      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                      (intermediate_act_fn): GELUActivation()\n",
       "                    )\n",
       "                    (output): SwinOutput(\n",
       "                      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (hidden_states_norms): ModuleDict(\n",
       "            (stage2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (stage3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (stage4): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (position_embedding): GroundingDinoSinePositionEmbedding()\n",
       "    )\n",
       "    (input_proj_vision): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (text_backbone): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (text_projection): Linear(in_features=768, out_features=256, bias=True)\n",
       "    (query_position_embeddings): Embedding(900, 256)\n",
       "    (encoder): GroundingDinoEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x GroundingDinoEncoderLayer(\n",
       "          (text_enhancer_layer): GroundingDinoTextEnhancerLayer(\n",
       "            (self_attn): GroundingDinoMultiheadAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (layer_norm_before): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (layer_norm_after): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation): ReLU()\n",
       "          )\n",
       "          (fusion_layer): GroundingDinoFusionLayer(\n",
       "            (layer_norm_vision): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (layer_norm_text): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GroundingDinoBiMultiHeadAttention(\n",
       "              (vision_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (text_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (values_vision_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (values_text_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (out_vision_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (out_text_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            )\n",
       "            (drop_path): GroundingDinoDropPath(p=0.1)\n",
       "          )\n",
       "          (deformable_layer): GroundingDinoDeformableLayer(\n",
       "            (self_attn): GroundingDinoMultiscaleDeformableAttention(\n",
       "              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): ReLU()\n",
       "            (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): GroundingDinoDecoder(\n",
       "      (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x GroundingDinoDecoderLayer(\n",
       "          (self_attn): GroundingDinoMultiheadAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn_text): GroundingDinoMultiheadAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (encoder_attn_text_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): GroundingDinoMultiscaleDeformableAttention(\n",
       "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (reference_points_head): GroundingDinoMLPPredictionHead(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (bbox_embed): ModuleList(\n",
       "        (0-5): 6 x GroundingDinoMLPPredictionHead(\n",
       "          (layers): ModuleList(\n",
       "            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (class_embed): ModuleList(\n",
       "        (0-5): 6 x GroundingDinoContrastiveEmbedding()\n",
       "      )\n",
       "    )\n",
       "    (enc_output): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder_output_bbox_embed): GroundingDinoMLPPredictionHead(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (encoder_output_class_embed): GroundingDinoContrastiveEmbedding()\n",
       "  )\n",
       "  (bbox_embed): ModuleList(\n",
       "    (0-5): 6 x GroundingDinoMLPPredictionHead(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (class_embed): ModuleList(\n",
       "    (0-5): 6 x GroundingDinoContrastiveEmbedding()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07469f72-a027-4485-a5d1-d729af38d239",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "image = Image.open(\"test.jpeg\")\n",
    "inputs = processor(images=image, text=\"a dog\", return_tensors=\"pt\").to(\"cuda\")\n",
    "times = {\"non_specific\": [], \"specific\": []}\n",
    "# starter, ender = cuda.Event(enable_timing=True), cuda.Event(enable_timing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b98ece9d-7e11-4757-9f3e-45933a2eafeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimerHook:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.start_time = 0\n",
    "        self.total_time = 0\n",
    "        \n",
    "    def start(self, module, input):\n",
    "        self.start_time = time.perf_counter()\n",
    "        \n",
    "    def end(self, module, input, output):\n",
    "        self.total_time += time.perf_counter() - self.start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa2b12b6-ee37-44fb-89ba-83e04b715e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_timer = TimerHook(\"backbone\")\n",
    "text_backbone_timer = TimerHook(\"text_backbone\")\n",
    "# Register hooks\n",
    "backbone_forward_hook = model.model.backbone.register_forward_pre_hook(backbone_timer.start)\n",
    "backbone_backward_hook = model.model.backbone.register_forward_hook(backbone_timer.end)\n",
    "\n",
    "text_backbone_forward_hook = model.model.text_backbone.register_forward_pre_hook(text_backbone_timer.start)\n",
    "text_backbone_backward_hook = model.model.text_backbone.register_forward_hook(text_backbone_timer.end)\n",
    "\n",
    "start_total = time.perf_counter()\n",
    "output = model(**inputs)\n",
    "total_time = time.perf_counter() - start_total\n",
    "\n",
    "# Remove hooks\n",
    "backbone_forward_hook.remove()\n",
    "backbone_backward_hook.remove()\n",
    "text_backbone_forward_hook.remove()\n",
    "text_backbone_backward_hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81d78882-a0d4-4ff1-9fdf-f787345a879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_video(model, video_path, query):\n",
    "    backbone_timer = TimerHook(\"backbone\")\n",
    "    text_backbone_timer = TimerHook(\"text_backbone\")\n",
    "    backbone_forward_hook = model.model.backbone.register_forward_pre_hook(backbone_timer.start)\n",
    "    backbone_backward_hook = model.model.backbone.register_forward_hook(backbone_timer.end)\n",
    "    text_backbone_forward_hook = model.model.text_backbone.register_forward_pre_hook(text_backbone_timer.start)\n",
    "    text_backbone_backward_hook = model.model.text_backbone.register_forward_hook(text_backbone_timer.end)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    total_time = 0\n",
    "    with torch.no_grad():\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = torch.from_numpy(frame).float().to(\"cuda\")\n",
    "            inputs = processor(images=frame, text=query, return_tensors=\"pt\").to(\"cuda\")\n",
    "            start_time = time.perf_counter()\n",
    "            _ = model(**inputs)\n",
    "            end_time = time.perf_counter() - start_time\n",
    "            total_time += end_time\n",
    "            frame_count += 1\n",
    "    cap.release()\n",
    "    backbone_forward_hook.remove()\n",
    "    backbone_backward_hook.remove()\n",
    "    text_backbone_forward_hook.remove()\n",
    "    text_backbone_backward_hook.remove()\n",
    "    return {\n",
    "        \"model\": model_id,\n",
    "        \"video\": video_path,\n",
    "        \"query\": query,\n",
    "        \"frame_count\": frame_count,\n",
    "        \"non-query specific part (s)\": text_backbone_timer.total_time + backbone_timer.total_time,\n",
    "        \"query specific part (s)\": total_time - (text_backbone_timer.total_time + backbone_timer.total_time),\n",
    "        \"non-query specific part (%)\": (text_backbone_timer.total_time + backbone_timer.total_time) / total_time * 100,\n",
    "        \"query specific part (%)\": (total_time - (text_backbone_timer.total_time + backbone_timer.total_time)) / total_time * 100\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f43f86dd-eaf9-410a-a199-86ecba89af82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'IDEA-Research/grounding-dino-tiny', 'video': './hong_kong_airport_demo_data.mp4', 'query': 'pink suitcase', 'frame_count': 2701, 'non-query specific part (s)': 42.16182706599557, 'query specific part (s)': 803.8611774740166, 'non-query specific part (%)': 4.983531989052616, 'query specific part (%)': 95.01646801094739}\n"
     ]
    }
   ],
   "source": [
    "data = run_video(model, './hong_kong_airport_demo_data.mp4', 'pink suitcase')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93ee4033-be0a-4582-be28-568bac4d5196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from mmengine.config import Config\n",
    "from mmengine.dataset import Compose\n",
    "from mmengine.runner import Runner\n",
    "from mmengine.runner.amp import autocast\n",
    "from mmyolo.registry import RUNNERS\n",
    "from torchvision.ops import nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82ed0fa4-b096-4e95-8827-d073f15ac0e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/24 00:11:00 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmyolo\" in the \"log_processor\" registry tree. As a workaround, the current \"log_processor\" registry in \"mmengine\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmyolo\" is a correct scope, or whether the registry is initialized.\n",
      "12/24 00:11:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.10.0 (default, Mar  3 2022, 09:58:08) [GCC 7.5.0]\n",
      "    CUDA available: False\n",
      "    MUSA available: False\n",
      "    numpy_random_seed: 791419896\n",
      "    GCC: gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "    PyTorch: 1.11.0\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 7.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.5.2 (Git Hash a9302535553c73243c632ad3c4c80beec3d19a1e)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.12.0\n",
      "    OpenCV: 4.10.0\n",
      "    MMEngine: 0.10.3\n",
      "\n",
      "Runtime environment:\n",
      "    cudnn_benchmark: True\n",
      "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 791419896\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "12/24 00:11:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Config:\n",
      "_backend_args = None\n",
      "_multiscale_resize_transforms = [\n",
      "    dict(\n",
      "        transforms=[\n",
      "            dict(scale=(\n",
      "                640,\n",
      "                640,\n",
      "            ), type='YOLOv5KeepRatioResize'),\n",
      "            dict(\n",
      "                allow_scale_up=False,\n",
      "                pad_val=dict(img=114),\n",
      "                scale=(\n",
      "                    640,\n",
      "                    640,\n",
      "                ),\n",
      "                type='LetterResize'),\n",
      "        ],\n",
      "        type='Compose'),\n",
      "    dict(\n",
      "        transforms=[\n",
      "            dict(scale=(\n",
      "                320,\n",
      "                320,\n",
      "            ), type='YOLOv5KeepRatioResize'),\n",
      "            dict(\n",
      "                allow_scale_up=False,\n",
      "                pad_val=dict(img=114),\n",
      "                scale=(\n",
      "                    320,\n",
      "                    320,\n",
      "                ),\n",
      "                type='LetterResize'),\n",
      "        ],\n",
      "        type='Compose'),\n",
      "    dict(\n",
      "        transforms=[\n",
      "            dict(scale=(\n",
      "                960,\n",
      "                960,\n",
      "            ), type='YOLOv5KeepRatioResize'),\n",
      "            dict(\n",
      "                allow_scale_up=False,\n",
      "                pad_val=dict(img=114),\n",
      "                scale=(\n",
      "                    960,\n",
      "                    960,\n",
      "                ),\n",
      "                type='LetterResize'),\n",
      "        ],\n",
      "        type='Compose'),\n",
      "]\n",
      "affine_scale = 0.9\n",
      "albu_train_transforms = [\n",
      "    dict(p=0.01, type='Blur'),\n",
      "    dict(p=0.01, type='MedianBlur'),\n",
      "    dict(p=0.01, type='ToGray'),\n",
      "    dict(p=0.01, type='CLAHE'),\n",
      "]\n",
      "backend_args = None\n",
      "base_lr = 0.002\n",
      "batch_shapes_cfg = None\n",
      "close_mosaic_epochs = 2\n",
      "coco_val_dataset = dict(\n",
      "    _delete_=True,\n",
      "    class_text_path='data/texts/lvis_v1_class_texts.json',\n",
      "    dataset=dict(\n",
      "        ann_file='lvis/lvis_v1_minival_inserted_image_name.json',\n",
      "        batch_shapes_cfg=None,\n",
      "        data_prefix=dict(img=''),\n",
      "        data_root='data/coco/',\n",
      "        test_mode=True,\n",
      "        type='YOLOv5LVISV1Dataset'),\n",
      "    pipeline=[\n",
      "        dict(type='LoadImageFromFile'),\n",
      "        dict(scale=(\n",
      "            1280,\n",
      "            1280,\n",
      "        ), type='YOLOv5KeepRatioResize'),\n",
      "        dict(\n",
      "            allow_scale_up=False,\n",
      "            pad_val=dict(img=114),\n",
      "            scale=(\n",
      "                1280,\n",
      "                1280,\n",
      "            ),\n",
      "            type='LetterResize'),\n",
      "        dict(_scope_='mmdet', type='LoadAnnotations', with_bbox=True),\n",
      "        dict(type='LoadText'),\n",
      "        dict(\n",
      "            meta_keys=(\n",
      "                'img_id',\n",
      "                'img_path',\n",
      "                'ori_shape',\n",
      "                'img_shape',\n",
      "                'scale_factor',\n",
      "                'pad_param',\n",
      "                'texts',\n",
      "            ),\n",
      "            type='mmdet.PackDetInputs'),\n",
      "    ],\n",
      "    type='MultiModalDataset')\n",
      "custom_hooks = [\n",
      "    dict(\n",
      "        ema_type='ExpMomentumEMA',\n",
      "        momentum=0.0001,\n",
      "        priority=49,\n",
      "        strict_load=False,\n",
      "        type='EMAHook',\n",
      "        update_buffers=True),\n",
      "    dict(\n",
      "        switch_epoch=98,\n",
      "        switch_pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(scale=(\n",
      "                1280,\n",
      "                1280,\n",
      "            ), type='YOLOv5KeepRatioResize'),\n",
      "            dict(\n",
      "                allow_scale_up=True,\n",
      "                pad_val=dict(img=114.0),\n",
      "                scale=(\n",
      "                    1280,\n",
      "                    1280,\n",
      "                ),\n",
      "                type='LetterResize'),\n",
      "            dict(\n",
      "                border_val=(\n",
      "                    114,\n",
      "                    114,\n",
      "                    114,\n",
      "                ),\n",
      "                max_aspect_ratio=100,\n",
      "                max_rotate_degree=0.0,\n",
      "                max_shear_degree=0.0,\n",
      "                scaling_ratio_range=(\n",
      "                    0.09999999999999998,\n",
      "                    1.9,\n",
      "                ),\n",
      "                type='YOLOv5RandomAffine'),\n",
      "            dict(\n",
      "                bbox_params=dict(\n",
      "                    format='pascal_voc',\n",
      "                    label_fields=[\n",
      "                        'gt_bboxes_labels',\n",
      "                        'gt_ignore_flags',\n",
      "                    ],\n",
      "                    type='BboxParams'),\n",
      "                keymap=dict(gt_bboxes='bboxes', img='image'),\n",
      "                transforms=[\n",
      "                    dict(p=0.01, type='Blur'),\n",
      "                    dict(p=0.01, type='MedianBlur'),\n",
      "                    dict(p=0.01, type='ToGray'),\n",
      "                    dict(p=0.01, type='CLAHE'),\n",
      "                ],\n",
      "                type='mmdet.Albu'),\n",
      "            dict(type='YOLOv5HSVRandomAug'),\n",
      "            dict(prob=0.5, type='mmdet.RandomFlip'),\n",
      "            dict(\n",
      "                max_num_samples=80,\n",
      "                num_neg_samples=(\n",
      "                    1203,\n",
      "                    1203,\n",
      "                ),\n",
      "                padding_to_max=True,\n",
      "                padding_value='',\n",
      "                type='RandomLoadText'),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'flip',\n",
      "                    'flip_direction',\n",
      "                    'texts',\n",
      "                ),\n",
      "                type='mmdet.PackDetInputs'),\n",
      "        ],\n",
      "        type='mmdet.PipelineSwitchHook'),\n",
      "]\n",
      "custom_imports = dict(\n",
      "    allow_failed_imports=False, imports=[\n",
      "        'yolo_world',\n",
      "    ])\n",
      "data_root = 'data/coco/'\n",
      "dataset_type = 'YOLOv5CocoDataset'\n",
      "deepen_factor = 1.0\n",
      "default_hooks = dict(\n",
      "    checkpoint=dict(\n",
      "        interval=2,\n",
      "        max_keep_ckpts=2,\n",
      "        rule='greater',\n",
      "        save_best='auto',\n",
      "        type='CheckpointHook'),\n",
      "    logger=dict(interval=50, type='LoggerHook'),\n",
      "    param_scheduler=dict(\n",
      "        lr_factor=0.01,\n",
      "        max_epochs=100,\n",
      "        scheduler_type='linear',\n",
      "        type='YOLOv5ParamSchedulerHook'),\n",
      "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
      "    timer=dict(type='IterTimerHook'),\n",
      "    visualization=dict(type='mmdet.DetVisualizationHook'))\n",
      "default_scope = 'mmyolo'\n",
      "env_cfg = dict(\n",
      "    cudnn_benchmark=True,\n",
      "    dist_cfg=dict(backend='nccl'),\n",
      "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
      "flickr_train_dataset = dict(\n",
      "    ann_file='annotations/final_flickr_separateGT_train.json',\n",
      "    data_prefix=dict(img='full_images/'),\n",
      "    data_root='data/flickr/',\n",
      "    filter_cfg=dict(filter_empty_gt=True, min_size=32),\n",
      "    pipeline=[\n",
      "        dict(backend_args=None, type='LoadImageFromFile'),\n",
      "        dict(type='LoadAnnotations', with_bbox=True),\n",
      "        dict(\n",
      "            img_scale=(\n",
      "                1280,\n",
      "                1280,\n",
      "            ),\n",
      "            pad_val=114.0,\n",
      "            pre_transform=[\n",
      "                dict(backend_args=None, type='LoadImageFromFile'),\n",
      "                dict(type='LoadAnnotations', with_bbox=True),\n",
      "            ],\n",
      "            type='MultiModalMosaic'),\n",
      "        dict(\n",
      "            border=(\n",
      "                -640,\n",
      "                -640,\n",
      "            ),\n",
      "            border_val=(\n",
      "                114,\n",
      "                114,\n",
      "                114,\n",
      "            ),\n",
      "            max_aspect_ratio=100,\n",
      "            max_rotate_degree=0.0,\n",
      "            max_shear_degree=0.0,\n",
      "            scaling_ratio_range=(\n",
      "                0.09999999999999998,\n",
      "                1.9,\n",
      "            ),\n",
      "            type='YOLOv5RandomAffine'),\n",
      "        dict(\n",
      "            bbox_params=dict(\n",
      "                format='pascal_voc',\n",
      "                label_fields=[\n",
      "                    'gt_bboxes_labels',\n",
      "                    'gt_ignore_flags',\n",
      "                ],\n",
      "                type='BboxParams'),\n",
      "            keymap=dict(gt_bboxes='bboxes', img='image'),\n",
      "            transforms=[\n",
      "                dict(p=0.01, type='Blur'),\n",
      "                dict(p=0.01, type='MedianBlur'),\n",
      "                dict(p=0.01, type='ToGray'),\n",
      "                dict(p=0.01, type='CLAHE'),\n",
      "            ],\n",
      "            type='mmdet.Albu'),\n",
      "        dict(type='YOLOv5HSVRandomAug'),\n",
      "        dict(prob=0.5, type='mmdet.RandomFlip'),\n",
      "        dict(\n",
      "            max_num_samples=80,\n",
      "            num_neg_samples=(\n",
      "                1203,\n",
      "                1203,\n",
      "            ),\n",
      "            padding_to_max=True,\n",
      "            padding_value='',\n",
      "            type='RandomLoadText'),\n",
      "        dict(\n",
      "            meta_keys=(\n",
      "                'img_id',\n",
      "                'img_path',\n",
      "                'ori_shape',\n",
      "                'img_shape',\n",
      "                'flip',\n",
      "                'flip_direction',\n",
      "                'texts',\n",
      "            ),\n",
      "            type='mmdet.PackDetInputs'),\n",
      "    ],\n",
      "    type='YOLOv5MixedGroundingDataset')\n",
      "img_scale = (\n",
      "    1280,\n",
      "    1280,\n",
      ")\n",
      "img_scales = [\n",
      "    (\n",
      "        640,\n",
      "        640,\n",
      "    ),\n",
      "    (\n",
      "        320,\n",
      "        320,\n",
      "    ),\n",
      "    (\n",
      "        960,\n",
      "        960,\n",
      "    ),\n",
      "]\n",
      "last_stage_out_channels = 512\n",
      "last_transform = [\n",
      "    dict(\n",
      "        bbox_params=dict(\n",
      "            format='pascal_voc',\n",
      "            label_fields=[\n",
      "                'gt_bboxes_labels',\n",
      "                'gt_ignore_flags',\n",
      "            ],\n",
      "            type='BboxParams'),\n",
      "        keymap=dict(gt_bboxes='bboxes', img='image'),\n",
      "        transforms=[\n",
      "            dict(p=0.01, type='Blur'),\n",
      "            dict(p=0.01, type='MedianBlur'),\n",
      "            dict(p=0.01, type='ToGray'),\n",
      "            dict(p=0.01, type='CLAHE'),\n",
      "        ],\n",
      "        type='mmdet.Albu'),\n",
      "    dict(type='YOLOv5HSVRandomAug'),\n",
      "    dict(prob=0.5, type='mmdet.RandomFlip'),\n",
      "    dict(\n",
      "        meta_keys=(\n",
      "            'img_id',\n",
      "            'img_path',\n",
      "            'ori_shape',\n",
      "            'img_shape',\n",
      "            'flip',\n",
      "            'flip_direction',\n",
      "        ),\n",
      "        type='mmdet.PackDetInputs'),\n",
      "]\n",
      "load_from = 'yolo_world_v2_x_obj365v1_goldg_cc3mlite_pretrain_1280ft-14996a36.pth'\n",
      "log_level = 'INFO'\n",
      "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)\n",
      "loss_bbox_weight = 7.5\n",
      "loss_cls_weight = 0.5\n",
      "loss_dfl_weight = 0.375\n",
      "lr_factor = 0.01\n",
      "max_aspect_ratio = 100\n",
      "max_epochs = 100\n",
      "max_keep_ckpts = 2\n",
      "mg_train_dataset = dict(\n",
      "    ann_file='annotations/final_mixed_train_no_coco.json',\n",
      "    data_prefix=dict(img='gqa/images/'),\n",
      "    data_root='data/mixed_grounding/',\n",
      "    filter_cfg=dict(filter_empty_gt=False, min_size=32),\n",
      "    pipeline=[\n",
      "        dict(backend_args=None, type='LoadImageFromFile'),\n",
      "        dict(type='LoadAnnotations', with_bbox=True),\n",
      "        dict(\n",
      "            img_scale=(\n",
      "                1280,\n",
      "                1280,\n",
      "            ),\n",
      "            pad_val=114.0,\n",
      "            pre_transform=[\n",
      "                dict(backend_args=None, type='LoadImageFromFile'),\n",
      "                dict(type='LoadAnnotations', with_bbox=True),\n",
      "            ],\n",
      "            type='MultiModalMosaic'),\n",
      "        dict(\n",
      "            border=(\n",
      "                -640,\n",
      "                -640,\n",
      "            ),\n",
      "            border_val=(\n",
      "                114,\n",
      "                114,\n",
      "                114,\n",
      "            ),\n",
      "            max_aspect_ratio=100,\n",
      "            max_rotate_degree=0.0,\n",
      "            max_shear_degree=0.0,\n",
      "            scaling_ratio_range=(\n",
      "                0.09999999999999998,\n",
      "                1.9,\n",
      "            ),\n",
      "            type='YOLOv5RandomAffine'),\n",
      "        dict(\n",
      "            bbox_params=dict(\n",
      "                format='pascal_voc',\n",
      "                label_fields=[\n",
      "                    'gt_bboxes_labels',\n",
      "                    'gt_ignore_flags',\n",
      "                ],\n",
      "                type='BboxParams'),\n",
      "            keymap=dict(gt_bboxes='bboxes', img='image'),\n",
      "            transforms=[\n",
      "                dict(p=0.01, type='Blur'),\n",
      "                dict(p=0.01, type='MedianBlur'),\n",
      "                dict(p=0.01, type='ToGray'),\n",
      "                dict(p=0.01, type='CLAHE'),\n",
      "            ],\n",
      "            type='mmdet.Albu'),\n",
      "        dict(type='YOLOv5HSVRandomAug'),\n",
      "        dict(prob=0.5, type='mmdet.RandomFlip'),\n",
      "        dict(\n",
      "            max_num_samples=80,\n",
      "            num_neg_samples=(\n",
      "                1203,\n",
      "                1203,\n",
      "            ),\n",
      "            padding_to_max=True,\n",
      "            padding_value='',\n",
      "            type='RandomLoadText'),\n",
      "        dict(\n",
      "            meta_keys=(\n",
      "                'img_id',\n",
      "                'img_path',\n",
      "                'ori_shape',\n",
      "                'img_shape',\n",
      "                'flip',\n",
      "                'flip_direction',\n",
      "                'texts',\n",
      "            ),\n",
      "            type='mmdet.PackDetInputs'),\n",
      "    ],\n",
      "    type='YOLOv5MixedGroundingDataset')\n",
      "mixup_prob = 0.15\n",
      "model = dict(\n",
      "    backbone=dict(\n",
      "        image_model=dict(\n",
      "            act_cfg=dict(inplace=True, type='SiLU'),\n",
      "            arch='P5',\n",
      "            deepen_factor=1.0,\n",
      "            last_stage_out_channels=512,\n",
      "            norm_cfg=dict(eps=0.001, momentum=0.03, type='BN'),\n",
      "            type='YOLOv8CSPDarknet',\n",
      "            widen_factor=1.25),\n",
      "        text_model=dict(\n",
      "            frozen_modules=[\n",
      "                'all',\n",
      "            ],\n",
      "            model_name='openai/clip-vit-base-patch32',\n",
      "            type='HuggingCLIPLanguageBackbone'),\n",
      "        type='MultiModalYOLOBackbone'),\n",
      "    bbox_head=dict(\n",
      "        bbox_coder=dict(type='DistancePointBBoxCoder'),\n",
      "        head_module=dict(\n",
      "            act_cfg=dict(inplace=True, type='SiLU'),\n",
      "            embed_dims=512,\n",
      "            featmap_strides=[\n",
      "                8,\n",
      "                16,\n",
      "                32,\n",
      "            ],\n",
      "            in_channels=[\n",
      "                256,\n",
      "                512,\n",
      "                512,\n",
      "            ],\n",
      "            norm_cfg=dict(eps=0.001, momentum=0.03, type='BN'),\n",
      "            num_classes=80,\n",
      "            reg_max=16,\n",
      "            type='YOLOWorldHeadModule',\n",
      "            use_bn_head=True,\n",
      "            widen_factor=1.25),\n",
      "        loss_bbox=dict(\n",
      "            bbox_format='xyxy',\n",
      "            iou_mode='ciou',\n",
      "            loss_weight=7.5,\n",
      "            reduction='sum',\n",
      "            return_iou=False,\n",
      "            type='IoULoss'),\n",
      "        loss_cls=dict(\n",
      "            loss_weight=0.5,\n",
      "            reduction='none',\n",
      "            type='mmdet.CrossEntropyLoss',\n",
      "            use_sigmoid=True),\n",
      "        loss_dfl=dict(\n",
      "            loss_weight=0.375,\n",
      "            reduction='mean',\n",
      "            type='mmdet.DistributionFocalLoss'),\n",
      "        prior_generator=dict(\n",
      "            offset=0.5, strides=[\n",
      "                8,\n",
      "                16,\n",
      "                32,\n",
      "            ], type='mmdet.MlvlPointGenerator'),\n",
      "        type='YOLOWorldHead'),\n",
      "    data_preprocessor=dict(\n",
      "        bgr_to_rgb=True,\n",
      "        mean=[\n",
      "            0.0,\n",
      "            0.0,\n",
      "            0.0,\n",
      "        ],\n",
      "        std=[\n",
      "            255.0,\n",
      "            255.0,\n",
      "            255.0,\n",
      "        ],\n",
      "        type='YOLOWDetDataPreprocessor'),\n",
      "    mm_neck=True,\n",
      "    neck=dict(\n",
      "        act_cfg=dict(inplace=True, type='SiLU'),\n",
      "        block_cfg=dict(type='MaxSigmoidCSPLayerWithTwoConv'),\n",
      "        deepen_factor=1.0,\n",
      "        embed_channels=[\n",
      "            128,\n",
      "            256,\n",
      "            256,\n",
      "        ],\n",
      "        guide_channels=512,\n",
      "        in_channels=[\n",
      "            256,\n",
      "            512,\n",
      "            512,\n",
      "        ],\n",
      "        norm_cfg=dict(eps=0.001, momentum=0.03, type='BN'),\n",
      "        num_csp_blocks=3,\n",
      "        num_heads=[\n",
      "            4,\n",
      "            8,\n",
      "            8,\n",
      "        ],\n",
      "        out_channels=[\n",
      "            256,\n",
      "            512,\n",
      "            512,\n",
      "        ],\n",
      "        type='YOLOWorldPAFPN',\n",
      "        widen_factor=1.25),\n",
      "    num_test_classes=1203,\n",
      "    num_train_classes=80,\n",
      "    test_cfg=dict(\n",
      "        max_per_img=300,\n",
      "        multi_label=True,\n",
      "        nms=dict(iou_threshold=0.7, type='nms'),\n",
      "        nms_pre=30000,\n",
      "        score_thr=0.001),\n",
      "    train_cfg=dict(\n",
      "        assigner=dict(\n",
      "            alpha=0.5,\n",
      "            beta=6.0,\n",
      "            eps=1e-09,\n",
      "            num_classes=80,\n",
      "            topk=10,\n",
      "            type='BatchTaskAlignedAssigner',\n",
      "            use_ciou=True)),\n",
      "    type='YOLOWorldDetector')\n",
      "model_test_cfg = dict(\n",
      "    max_per_img=300,\n",
      "    multi_label=True,\n",
      "    nms=dict(iou_threshold=0.7, type='nms'),\n",
      "    nms_pre=30000,\n",
      "    score_thr=0.001)\n",
      "mosaic_affine_transform = [\n",
      "    dict(\n",
      "        img_scale=(\n",
      "            640,\n",
      "            640,\n",
      "        ),\n",
      "        pad_val=114.0,\n",
      "        pre_transform=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "        ],\n",
      "        type='Mosaic'),\n",
      "    dict(\n",
      "        border=(\n",
      "            -320,\n",
      "            -320,\n",
      "        ),\n",
      "        border_val=(\n",
      "            114,\n",
      "            114,\n",
      "            114,\n",
      "        ),\n",
      "        max_aspect_ratio=100,\n",
      "        max_rotate_degree=0.0,\n",
      "        max_shear_degree=0.0,\n",
      "        scaling_ratio_range=(\n",
      "            0.09999999999999998,\n",
      "            1.9,\n",
      "        ),\n",
      "        type='YOLOv5RandomAffine'),\n",
      "]\n",
      "neck_embed_channels = [\n",
      "    128,\n",
      "    256,\n",
      "    256,\n",
      "]\n",
      "neck_num_heads = [\n",
      "    4,\n",
      "    8,\n",
      "    8,\n",
      "]\n",
      "norm_cfg = dict(eps=0.001, momentum=0.03, type='BN')\n",
      "num_classes = 1203\n",
      "num_det_layers = 3\n",
      "num_training_classes = 80\n",
      "obj365v1_train_dataset = dict(\n",
      "    class_text_path='data/texts/obj365v1_class_texts.json',\n",
      "    dataset=dict(\n",
      "        ann_file='annotations/objects365_train.json',\n",
      "        data_prefix=dict(img='train/'),\n",
      "        data_root='data/objects365v1/',\n",
      "        filter_cfg=dict(filter_empty_gt=False, min_size=32),\n",
      "        type='YOLOv5Objects365V1Dataset'),\n",
      "    pipeline=[\n",
      "        dict(backend_args=None, type='LoadImageFromFile'),\n",
      "        dict(type='LoadAnnotations', with_bbox=True),\n",
      "        dict(\n",
      "            img_scale=(\n",
      "                1280,\n",
      "                1280,\n",
      "            ),\n",
      "            pad_val=114.0,\n",
      "            pre_transform=[\n",
      "                dict(backend_args=None, type='LoadImageFromFile'),\n",
      "                dict(type='LoadAnnotations', with_bbox=True),\n",
      "            ],\n",
      "            type='MultiModalMosaic'),\n",
      "        dict(\n",
      "            border=(\n",
      "                -640,\n",
      "                -640,\n",
      "            ),\n",
      "            border_val=(\n",
      "                114,\n",
      "                114,\n",
      "                114,\n",
      "            ),\n",
      "            max_aspect_ratio=100,\n",
      "            max_rotate_degree=0.0,\n",
      "            max_shear_degree=0.0,\n",
      "            scaling_ratio_range=(\n",
      "                0.09999999999999998,\n",
      "                1.9,\n",
      "            ),\n",
      "            type='YOLOv5RandomAffine'),\n",
      "        dict(\n",
      "            bbox_params=dict(\n",
      "                format='pascal_voc',\n",
      "                label_fields=[\n",
      "                    'gt_bboxes_labels',\n",
      "                    'gt_ignore_flags',\n",
      "                ],\n",
      "                type='BboxParams'),\n",
      "            keymap=dict(gt_bboxes='bboxes', img='image'),\n",
      "            transforms=[\n",
      "                dict(p=0.01, type='Blur'),\n",
      "                dict(p=0.01, type='MedianBlur'),\n",
      "                dict(p=0.01, type='ToGray'),\n",
      "                dict(p=0.01, type='CLAHE'),\n",
      "            ],\n",
      "            type='mmdet.Albu'),\n",
      "        dict(type='YOLOv5HSVRandomAug'),\n",
      "        dict(prob=0.5, type='mmdet.RandomFlip'),\n",
      "        dict(\n",
      "            max_num_samples=80,\n",
      "            num_neg_samples=(\n",
      "                1203,\n",
      "                1203,\n",
      "            ),\n",
      "            padding_to_max=True,\n",
      "            padding_value='',\n",
      "            type='RandomLoadText'),\n",
      "        dict(\n",
      "            meta_keys=(\n",
      "                'img_id',\n",
      "                'img_path',\n",
      "                'ori_shape',\n",
      "                'img_shape',\n",
      "                'flip',\n",
      "                'flip_direction',\n",
      "                'texts',\n",
      "            ),\n",
      "            type='mmdet.PackDetInputs'),\n",
      "    ],\n",
      "    type='MultiModalDataset')\n",
      "optim_wrapper = dict(\n",
      "    clip_grad=dict(max_norm=10.0),\n",
      "    constructor='YOLOWv5OptimizerConstructor',\n",
      "    optimizer=dict(\n",
      "        batch_size_per_gpu=16, lr=0.002, type='AdamW', weight_decay=0.025),\n",
      "    paramwise_cfg=dict(\n",
      "        bias_decay_mult=0.0,\n",
      "        custom_keys=dict({\n",
      "            'backbone.text_model': dict(lr_mult=0.01),\n",
      "            'logit_scale': dict(weight_decay=0.0)\n",
      "        }),\n",
      "        norm_decay_mult=0.0),\n",
      "    type='OptimWrapper')\n",
      "param_scheduler = None\n",
      "persistent_workers = True\n",
      "pre_transform = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "]\n",
      "resume = False\n",
      "save_epoch_intervals = 2\n",
      "strides = [\n",
      "    8,\n",
      "    16,\n",
      "    32,\n",
      "]\n",
      "tal_alpha = 0.5\n",
      "tal_beta = 6.0\n",
      "tal_topk = 10\n",
      "test_cfg = dict(type='TestLoop')\n",
      "test_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        class_text_path='data/texts/lvis_v1_class_texts.json',\n",
      "        dataset=dict(\n",
      "            ann_file='lvis/lvis_v1_minival_inserted_image_name.json',\n",
      "            batch_shapes_cfg=None,\n",
      "            data_prefix=dict(img=''),\n",
      "            data_root='data/coco/',\n",
      "            test_mode=True,\n",
      "            type='YOLOv5LVISV1Dataset'),\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(scale=(\n",
      "                1280,\n",
      "                1280,\n",
      "            ), type='YOLOv5KeepRatioResize'),\n",
      "            dict(\n",
      "                allow_scale_up=False,\n",
      "                pad_val=dict(img=114),\n",
      "                scale=(\n",
      "                    1280,\n",
      "                    1280,\n",
      "                ),\n",
      "                type='LetterResize'),\n",
      "            dict(_scope_='mmdet', type='LoadAnnotations', with_bbox=True),\n",
      "            dict(type='LoadText'),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'scale_factor',\n",
      "                    'pad_param',\n",
      "                    'texts',\n",
      "                ),\n",
      "                type='mmdet.PackDetInputs'),\n",
      "        ],\n",
      "        type='MultiModalDataset'),\n",
      "    drop_last=False,\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    pin_memory=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "test_evaluator = dict(\n",
      "    ann_file='data/coco/lvis/lvis_v1_minival_inserted_image_name.json',\n",
      "    metric='bbox',\n",
      "    proposal_nums=(\n",
      "        100,\n",
      "        1,\n",
      "        10,\n",
      "    ),\n",
      "    type='mmdet.LVISMetric')\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(scale=(\n",
      "        1280,\n",
      "        1280,\n",
      "    ), type='YOLOv5KeepRatioResize'),\n",
      "    dict(\n",
      "        allow_scale_up=False,\n",
      "        pad_val=dict(img=114),\n",
      "        scale=(\n",
      "            1280,\n",
      "            1280,\n",
      "        ),\n",
      "        type='LetterResize'),\n",
      "    dict(_scope_='mmdet', type='LoadAnnotations', with_bbox=True),\n",
      "    dict(type='LoadText'),\n",
      "    dict(\n",
      "        meta_keys=(\n",
      "            'img_id',\n",
      "            'img_path',\n",
      "            'ori_shape',\n",
      "            'img_shape',\n",
      "            'scale_factor',\n",
      "            'pad_param',\n",
      "            'texts',\n",
      "        ),\n",
      "        type='mmdet.PackDetInputs'),\n",
      "]\n",
      "text_channels = 512\n",
      "text_model_name = 'openai/clip-vit-base-patch32'\n",
      "text_transform = [\n",
      "    dict(\n",
      "        max_num_samples=80,\n",
      "        num_neg_samples=(\n",
      "            1203,\n",
      "            1203,\n",
      "        ),\n",
      "        padding_to_max=True,\n",
      "        padding_value='',\n",
      "        type='RandomLoadText'),\n",
      "    dict(\n",
      "        meta_keys=(\n",
      "            'img_id',\n",
      "            'img_path',\n",
      "            'ori_shape',\n",
      "            'img_shape',\n",
      "            'flip',\n",
      "            'flip_direction',\n",
      "            'texts',\n",
      "        ),\n",
      "        type='mmdet.PackDetInputs'),\n",
      "]\n",
      "train_ann_file = 'annotations/instances_train2017.json'\n",
      "train_batch_size_per_gpu = 16\n",
      "train_cfg = dict(\n",
      "    dynamic_intervals=[\n",
      "        (\n",
      "            98,\n",
      "            1,\n",
      "        ),\n",
      "    ],\n",
      "    max_epochs=100,\n",
      "    type='EpochBasedTrainLoop',\n",
      "    val_interval=10)\n",
      "train_data_prefix = 'train2017/'\n",
      "train_dataloader = dict(\n",
      "    batch_size=16,\n",
      "    collate_fn=dict(type='yolow_collate'),\n",
      "    dataset=dict(\n",
      "        datasets=[\n",
      "            dict(\n",
      "                class_text_path='data/texts/obj365v1_class_texts.json',\n",
      "                dataset=dict(\n",
      "                    ann_file='annotations/objects365_train.json',\n",
      "                    data_prefix=dict(img='train/'),\n",
      "                    data_root='data/objects365v1/',\n",
      "                    filter_cfg=dict(filter_empty_gt=False, min_size=32),\n",
      "                    type='YOLOv5Objects365V1Dataset'),\n",
      "                pipeline=[\n",
      "                    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "                    dict(type='LoadAnnotations', with_bbox=True),\n",
      "                    dict(\n",
      "                        img_scale=(\n",
      "                            1280,\n",
      "                            1280,\n",
      "                        ),\n",
      "                        pad_val=114.0,\n",
      "                        pre_transform=[\n",
      "                            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "                            dict(type='LoadAnnotations', with_bbox=True),\n",
      "                        ],\n",
      "                        type='MultiModalMosaic'),\n",
      "                    dict(\n",
      "                        border=(\n",
      "                            -640,\n",
      "                            -640,\n",
      "                        ),\n",
      "                        border_val=(\n",
      "                            114,\n",
      "                            114,\n",
      "                            114,\n",
      "                        ),\n",
      "                        max_aspect_ratio=100,\n",
      "                        max_rotate_degree=0.0,\n",
      "                        max_shear_degree=0.0,\n",
      "                        scaling_ratio_range=(\n",
      "                            0.09999999999999998,\n",
      "                            1.9,\n",
      "                        ),\n",
      "                        type='YOLOv5RandomAffine'),\n",
      "                    dict(\n",
      "                        bbox_params=dict(\n",
      "                            format='pascal_voc',\n",
      "                            label_fields=[\n",
      "                                'gt_bboxes_labels',\n",
      "                                'gt_ignore_flags',\n",
      "                            ],\n",
      "                            type='BboxParams'),\n",
      "                        keymap=dict(gt_bboxes='bboxes', img='image'),\n",
      "                        transforms=[\n",
      "                            dict(p=0.01, type='Blur'),\n",
      "                            dict(p=0.01, type='MedianBlur'),\n",
      "                            dict(p=0.01, type='ToGray'),\n",
      "                            dict(p=0.01, type='CLAHE'),\n",
      "                        ],\n",
      "                        type='mmdet.Albu'),\n",
      "                    dict(type='YOLOv5HSVRandomAug'),\n",
      "                    dict(prob=0.5, type='mmdet.RandomFlip'),\n",
      "                    dict(\n",
      "                        max_num_samples=80,\n",
      "                        num_neg_samples=(\n",
      "                            1203,\n",
      "                            1203,\n",
      "                        ),\n",
      "                        padding_to_max=True,\n",
      "                        padding_value='',\n",
      "                        type='RandomLoadText'),\n",
      "                    dict(\n",
      "                        meta_keys=(\n",
      "                            'img_id',\n",
      "                            'img_path',\n",
      "                            'ori_shape',\n",
      "                            'img_shape',\n",
      "                            'flip',\n",
      "                            'flip_direction',\n",
      "                            'texts',\n",
      "                        ),\n",
      "                        type='mmdet.PackDetInputs'),\n",
      "                ],\n",
      "                type='MultiModalDataset'),\n",
      "            dict(\n",
      "                ann_file='annotations/final_flickr_separateGT_train.json',\n",
      "                data_prefix=dict(img='full_images/'),\n",
      "                data_root='data/flickr/',\n",
      "                filter_cfg=dict(filter_empty_gt=True, min_size=32),\n",
      "                pipeline=[\n",
      "                    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "                    dict(type='LoadAnnotations', with_bbox=True),\n",
      "                    dict(\n",
      "                        img_scale=(\n",
      "                            1280,\n",
      "                            1280,\n",
      "                        ),\n",
      "                        pad_val=114.0,\n",
      "                        pre_transform=[\n",
      "                            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "                            dict(type='LoadAnnotations', with_bbox=True),\n",
      "                        ],\n",
      "                        type='MultiModalMosaic'),\n",
      "                    dict(\n",
      "                        border=(\n",
      "                            -640,\n",
      "                            -640,\n",
      "                        ),\n",
      "                        border_val=(\n",
      "                            114,\n",
      "                            114,\n",
      "                            114,\n",
      "                        ),\n",
      "                        max_aspect_ratio=100,\n",
      "                        max_rotate_degree=0.0,\n",
      "                        max_shear_degree=0.0,\n",
      "                        scaling_ratio_range=(\n",
      "                            0.09999999999999998,\n",
      "                            1.9,\n",
      "                        ),\n",
      "                        type='YOLOv5RandomAffine'),\n",
      "                    dict(\n",
      "                        bbox_params=dict(\n",
      "                            format='pascal_voc',\n",
      "                            label_fields=[\n",
      "                                'gt_bboxes_labels',\n",
      "                                'gt_ignore_flags',\n",
      "                            ],\n",
      "                            type='BboxParams'),\n",
      "                        keymap=dict(gt_bboxes='bboxes', img='image'),\n",
      "                        transforms=[\n",
      "                            dict(p=0.01, type='Blur'),\n",
      "                            dict(p=0.01, type='MedianBlur'),\n",
      "                            dict(p=0.01, type='ToGray'),\n",
      "                            dict(p=0.01, type='CLAHE'),\n",
      "                        ],\n",
      "                        type='mmdet.Albu'),\n",
      "                    dict(type='YOLOv5HSVRandomAug'),\n",
      "                    dict(prob=0.5, type='mmdet.RandomFlip'),\n",
      "                    dict(\n",
      "                        max_num_samples=80,\n",
      "                        num_neg_samples=(\n",
      "                            1203,\n",
      "                            1203,\n",
      "                        ),\n",
      "                        padding_to_max=True,\n",
      "                        padding_value='',\n",
      "                        type='RandomLoadText'),\n",
      "                    dict(\n",
      "                        meta_keys=(\n",
      "                            'img_id',\n",
      "                            'img_path',\n",
      "                            'ori_shape',\n",
      "                            'img_shape',\n",
      "                            'flip',\n",
      "                            'flip_direction',\n",
      "                            'texts',\n",
      "                        ),\n",
      "                        type='mmdet.PackDetInputs'),\n",
      "                ],\n",
      "                type='YOLOv5MixedGroundingDataset'),\n",
      "            dict(\n",
      "                ann_file='annotations/final_mixed_train_no_coco.json',\n",
      "                data_prefix=dict(img='gqa/images/'),\n",
      "                data_root='data/mixed_grounding/',\n",
      "                filter_cfg=dict(filter_empty_gt=False, min_size=32),\n",
      "                pipeline=[\n",
      "                    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "                    dict(type='LoadAnnotations', with_bbox=True),\n",
      "                    dict(\n",
      "                        img_scale=(\n",
      "                            1280,\n",
      "                            1280,\n",
      "                        ),\n",
      "                        pad_val=114.0,\n",
      "                        pre_transform=[\n",
      "                            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "                            dict(type='LoadAnnotations', with_bbox=True),\n",
      "                        ],\n",
      "                        type='MultiModalMosaic'),\n",
      "                    dict(\n",
      "                        border=(\n",
      "                            -640,\n",
      "                            -640,\n",
      "                        ),\n",
      "                        border_val=(\n",
      "                            114,\n",
      "                            114,\n",
      "                            114,\n",
      "                        ),\n",
      "                        max_aspect_ratio=100,\n",
      "                        max_rotate_degree=0.0,\n",
      "                        max_shear_degree=0.0,\n",
      "                        scaling_ratio_range=(\n",
      "                            0.09999999999999998,\n",
      "                            1.9,\n",
      "                        ),\n",
      "                        type='YOLOv5RandomAffine'),\n",
      "                    dict(\n",
      "                        bbox_params=dict(\n",
      "                            format='pascal_voc',\n",
      "                            label_fields=[\n",
      "                                'gt_bboxes_labels',\n",
      "                                'gt_ignore_flags',\n",
      "                            ],\n",
      "                            type='BboxParams'),\n",
      "                        keymap=dict(gt_bboxes='bboxes', img='image'),\n",
      "                        transforms=[\n",
      "                            dict(p=0.01, type='Blur'),\n",
      "                            dict(p=0.01, type='MedianBlur'),\n",
      "                            dict(p=0.01, type='ToGray'),\n",
      "                            dict(p=0.01, type='CLAHE'),\n",
      "                        ],\n",
      "                        type='mmdet.Albu'),\n",
      "                    dict(type='YOLOv5HSVRandomAug'),\n",
      "                    dict(prob=0.5, type='mmdet.RandomFlip'),\n",
      "                    dict(\n",
      "                        max_num_samples=80,\n",
      "                        num_neg_samples=(\n",
      "                            1203,\n",
      "                            1203,\n",
      "                        ),\n",
      "                        padding_to_max=True,\n",
      "                        padding_value='',\n",
      "                        type='RandomLoadText'),\n",
      "                    dict(\n",
      "                        meta_keys=(\n",
      "                            'img_id',\n",
      "                            'img_path',\n",
      "                            'ori_shape',\n",
      "                            'img_shape',\n",
      "                            'flip',\n",
      "                            'flip_direction',\n",
      "                            'texts',\n",
      "                        ),\n",
      "                        type='mmdet.PackDetInputs'),\n",
      "                ],\n",
      "                type='YOLOv5MixedGroundingDataset'),\n",
      "        ],\n",
      "        ignore_keys=[\n",
      "            'classes',\n",
      "            'palette',\n",
      "        ],\n",
      "        type='ConcatDataset'),\n",
      "    num_workers=8,\n",
      "    persistent_workers=True,\n",
      "    pin_memory=True,\n",
      "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
      "train_num_workers = 8\n",
      "train_pipeline = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(\n",
      "        img_scale=(\n",
      "            1280,\n",
      "            1280,\n",
      "        ),\n",
      "        pad_val=114.0,\n",
      "        pre_transform=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "        ],\n",
      "        type='MultiModalMosaic'),\n",
      "    dict(\n",
      "        border=(\n",
      "            -640,\n",
      "            -640,\n",
      "        ),\n",
      "        border_val=(\n",
      "            114,\n",
      "            114,\n",
      "            114,\n",
      "        ),\n",
      "        max_aspect_ratio=100,\n",
      "        max_rotate_degree=0.0,\n",
      "        max_shear_degree=0.0,\n",
      "        scaling_ratio_range=(\n",
      "            0.09999999999999998,\n",
      "            1.9,\n",
      "        ),\n",
      "        type='YOLOv5RandomAffine'),\n",
      "    dict(\n",
      "        bbox_params=dict(\n",
      "            format='pascal_voc',\n",
      "            label_fields=[\n",
      "                'gt_bboxes_labels',\n",
      "                'gt_ignore_flags',\n",
      "            ],\n",
      "            type='BboxParams'),\n",
      "        keymap=dict(gt_bboxes='bboxes', img='image'),\n",
      "        transforms=[\n",
      "            dict(p=0.01, type='Blur'),\n",
      "            dict(p=0.01, type='MedianBlur'),\n",
      "            dict(p=0.01, type='ToGray'),\n",
      "            dict(p=0.01, type='CLAHE'),\n",
      "        ],\n",
      "        type='mmdet.Albu'),\n",
      "    dict(type='YOLOv5HSVRandomAug'),\n",
      "    dict(prob=0.5, type='mmdet.RandomFlip'),\n",
      "    dict(\n",
      "        max_num_samples=80,\n",
      "        num_neg_samples=(\n",
      "            1203,\n",
      "            1203,\n",
      "        ),\n",
      "        padding_to_max=True,\n",
      "        padding_value='',\n",
      "        type='RandomLoadText'),\n",
      "    dict(\n",
      "        meta_keys=(\n",
      "            'img_id',\n",
      "            'img_path',\n",
      "            'ori_shape',\n",
      "            'img_shape',\n",
      "            'flip',\n",
      "            'flip_direction',\n",
      "            'texts',\n",
      "        ),\n",
      "        type='mmdet.PackDetInputs'),\n",
      "]\n",
      "train_pipeline_stage2 = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(scale=(\n",
      "        1280,\n",
      "        1280,\n",
      "    ), type='YOLOv5KeepRatioResize'),\n",
      "    dict(\n",
      "        allow_scale_up=True,\n",
      "        pad_val=dict(img=114.0),\n",
      "        scale=(\n",
      "            1280,\n",
      "            1280,\n",
      "        ),\n",
      "        type='LetterResize'),\n",
      "    dict(\n",
      "        border_val=(\n",
      "            114,\n",
      "            114,\n",
      "            114,\n",
      "        ),\n",
      "        max_aspect_ratio=100,\n",
      "        max_rotate_degree=0.0,\n",
      "        max_shear_degree=0.0,\n",
      "        scaling_ratio_range=(\n",
      "            0.09999999999999998,\n",
      "            1.9,\n",
      "        ),\n",
      "        type='YOLOv5RandomAffine'),\n",
      "    dict(\n",
      "        bbox_params=dict(\n",
      "            format='pascal_voc',\n",
      "            label_fields=[\n",
      "                'gt_bboxes_labels',\n",
      "                'gt_ignore_flags',\n",
      "            ],\n",
      "            type='BboxParams'),\n",
      "        keymap=dict(gt_bboxes='bboxes', img='image'),\n",
      "        transforms=[\n",
      "            dict(p=0.01, type='Blur'),\n",
      "            dict(p=0.01, type='MedianBlur'),\n",
      "            dict(p=0.01, type='ToGray'),\n",
      "            dict(p=0.01, type='CLAHE'),\n",
      "        ],\n",
      "        type='mmdet.Albu'),\n",
      "    dict(type='YOLOv5HSVRandomAug'),\n",
      "    dict(prob=0.5, type='mmdet.RandomFlip'),\n",
      "    dict(\n",
      "        max_num_samples=80,\n",
      "        num_neg_samples=(\n",
      "            1203,\n",
      "            1203,\n",
      "        ),\n",
      "        padding_to_max=True,\n",
      "        padding_value='',\n",
      "        type='RandomLoadText'),\n",
      "    dict(\n",
      "        meta_keys=(\n",
      "            'img_id',\n",
      "            'img_path',\n",
      "            'ori_shape',\n",
      "            'img_shape',\n",
      "            'flip',\n",
      "            'flip_direction',\n",
      "            'texts',\n",
      "        ),\n",
      "        type='mmdet.PackDetInputs'),\n",
      "]\n",
      "tta_model = dict(\n",
      "    tta_cfg=dict(max_per_img=300, nms=dict(iou_threshold=0.65, type='nms')),\n",
      "    type='mmdet.DetTTAModel')\n",
      "tta_pipeline = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        transforms=[\n",
      "            [\n",
      "                dict(\n",
      "                    transforms=[\n",
      "                        dict(scale=(\n",
      "                            640,\n",
      "                            640,\n",
      "                        ), type='YOLOv5KeepRatioResize'),\n",
      "                        dict(\n",
      "                            allow_scale_up=False,\n",
      "                            pad_val=dict(img=114),\n",
      "                            scale=(\n",
      "                                640,\n",
      "                                640,\n",
      "                            ),\n",
      "                            type='LetterResize'),\n",
      "                    ],\n",
      "                    type='Compose'),\n",
      "                dict(\n",
      "                    transforms=[\n",
      "                        dict(scale=(\n",
      "                            320,\n",
      "                            320,\n",
      "                        ), type='YOLOv5KeepRatioResize'),\n",
      "                        dict(\n",
      "                            allow_scale_up=False,\n",
      "                            pad_val=dict(img=114),\n",
      "                            scale=(\n",
      "                                320,\n",
      "                                320,\n",
      "                            ),\n",
      "                            type='LetterResize'),\n",
      "                    ],\n",
      "                    type='Compose'),\n",
      "                dict(\n",
      "                    transforms=[\n",
      "                        dict(scale=(\n",
      "                            960,\n",
      "                            960,\n",
      "                        ), type='YOLOv5KeepRatioResize'),\n",
      "                        dict(\n",
      "                            allow_scale_up=False,\n",
      "                            pad_val=dict(img=114),\n",
      "                            scale=(\n",
      "                                960,\n",
      "                                960,\n",
      "                            ),\n",
      "                            type='LetterResize'),\n",
      "                    ],\n",
      "                    type='Compose'),\n",
      "            ],\n",
      "            [\n",
      "                dict(prob=1.0, type='mmdet.RandomFlip'),\n",
      "                dict(prob=0.0, type='mmdet.RandomFlip'),\n",
      "            ],\n",
      "            [\n",
      "                dict(type='mmdet.LoadAnnotations', with_bbox=True),\n",
      "            ],\n",
      "            [\n",
      "                dict(\n",
      "                    meta_keys=(\n",
      "                        'img_id',\n",
      "                        'img_path',\n",
      "                        'ori_shape',\n",
      "                        'img_shape',\n",
      "                        'scale_factor',\n",
      "                        'pad_param',\n",
      "                        'flip',\n",
      "                        'flip_direction',\n",
      "                    ),\n",
      "                    type='mmdet.PackDetInputs'),\n",
      "            ],\n",
      "        ],\n",
      "        type='TestTimeAug'),\n",
      "]\n",
      "val_ann_file = 'annotations/instances_val2017.json'\n",
      "val_batch_size_per_gpu = 1\n",
      "val_cfg = dict(type='ValLoop')\n",
      "val_data_prefix = 'val2017/'\n",
      "val_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        class_text_path='data/texts/lvis_v1_class_texts.json',\n",
      "        dataset=dict(\n",
      "            ann_file='lvis/lvis_v1_minival_inserted_image_name.json',\n",
      "            batch_shapes_cfg=None,\n",
      "            data_prefix=dict(img=''),\n",
      "            data_root='data/coco/',\n",
      "            test_mode=True,\n",
      "            type='YOLOv5LVISV1Dataset'),\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(scale=(\n",
      "                1280,\n",
      "                1280,\n",
      "            ), type='YOLOv5KeepRatioResize'),\n",
      "            dict(\n",
      "                allow_scale_up=False,\n",
      "                pad_val=dict(img=114),\n",
      "                scale=(\n",
      "                    1280,\n",
      "                    1280,\n",
      "                ),\n",
      "                type='LetterResize'),\n",
      "            dict(_scope_='mmdet', type='LoadAnnotations', with_bbox=True),\n",
      "            dict(type='LoadText'),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'scale_factor',\n",
      "                    'pad_param',\n",
      "                    'texts',\n",
      "                ),\n",
      "                type='mmdet.PackDetInputs'),\n",
      "        ],\n",
      "        type='MultiModalDataset'),\n",
      "    drop_last=False,\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    pin_memory=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "val_evaluator = dict(\n",
      "    ann_file='data/coco/lvis/lvis_v1_minival_inserted_image_name.json',\n",
      "    metric='bbox',\n",
      "    proposal_nums=(\n",
      "        100,\n",
      "        1,\n",
      "        10,\n",
      "    ),\n",
      "    type='mmdet.LVISMetric')\n",
      "val_interval_stage2 = 1\n",
      "val_num_workers = 2\n",
      "vis_backends = [\n",
      "    dict(type='LocalVisBackend'),\n",
      "]\n",
      "visualizer = dict(\n",
      "    name='visualizer',\n",
      "    type='mmdet.DetLocalVisualizer',\n",
      "    vis_backends=[\n",
      "        dict(type='LocalVisBackend'),\n",
      "    ])\n",
      "weight_decay = 0.025\n",
      "widen_factor = 1.25\n",
      "work_dir = '.'\n",
      "\n",
      "12/24 00:11:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "12/24 00:11:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(49          ) EMAHook                            \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_load_checkpoint:\n",
      "(49          ) EMAHook                            \n",
      " -------------------- \n",
      "before_train:\n",
      "(9           ) YOLOv5ParamSchedulerHook           \n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(49          ) EMAHook                            \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      "(NORMAL      ) PipelineSwitchHook                 \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(9           ) YOLOv5ParamSchedulerHook           \n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(9           ) YOLOv5ParamSchedulerHook           \n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(49          ) EMAHook                            \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(9           ) YOLOv5ParamSchedulerHook           \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(49          ) EMAHook                            \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DetVisualizationHook               \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(9           ) YOLOv5ParamSchedulerHook           \n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(49          ) EMAHook                            \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_save_checkpoint:\n",
      "(49          ) EMAHook                            \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(49          ) EMAHook                            \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DetVisualizationHook               \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(49          ) EMAHook                            \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "Loads checkpoint by local backend from path: yolo_world_v2_x_obj365v1_goldg_cc3mlite_pretrain_1280ft-14996a36.pth\n",
      "12/24 00:11:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Load checkpoint from yolo_world_v2_x_obj365v1_goldg_cc3mlite_pretrain_1280ft-14996a36.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "YOLOWorldDetector(\n",
       "  (data_preprocessor): YOLOWDetDataPreprocessor()\n",
       "  (backbone): MultiModalYOLOBackbone(\n",
       "    (image_model): YOLOv8CSPDarknet(\n",
       "      (stem): ConvModule(\n",
       "        (conv): Conv2d(3, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (activate): SiLU(inplace=True)\n",
       "      )\n",
       "      (stage1): Sequential(\n",
       "        (0): ConvModule(\n",
       "          (conv): Conv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (activate): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): CSPLayerWithTwoConv(\n",
       "          (main_conv): ConvModule(\n",
       "            (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (final_conv): ConvModule(\n",
       "            (conv): Conv2d(400, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (blocks): ModuleList(\n",
       "            (0): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (stage2): Sequential(\n",
       "        (0): ConvModule(\n",
       "          (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (activate): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): CSPLayerWithTwoConv(\n",
       "          (main_conv): ConvModule(\n",
       "            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (final_conv): ConvModule(\n",
       "            (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (blocks): ModuleList(\n",
       "            (0): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (3): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (4): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (5): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (stage3): Sequential(\n",
       "        (0): ConvModule(\n",
       "          (conv): Conv2d(320, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (activate): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): CSPLayerWithTwoConv(\n",
       "          (main_conv): ConvModule(\n",
       "            (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (final_conv): ConvModule(\n",
       "            (conv): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (blocks): ModuleList(\n",
       "            (0): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (3): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (4): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (5): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (stage4): Sequential(\n",
       "        (0): ConvModule(\n",
       "          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (activate): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): CSPLayerWithTwoConv(\n",
       "          (main_conv): ConvModule(\n",
       "            (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (final_conv): ConvModule(\n",
       "            (conv): Conv2d(1600, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (blocks): ModuleList(\n",
       "            (0): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): SPPFBottleneck(\n",
       "          (conv1): ConvModule(\n",
       "            (conv): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (poolings): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "          (conv2): ConvModule(\n",
       "            (conv): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (text_model): HuggingCLIPLanguageBackbone(\n",
       "      (model): CLIPTextModelWithProjection(\n",
       "        (text_model): CLIPTextTransformer(\n",
       "          (embeddings): CLIPTextEmbeddings(\n",
       "            (token_embedding): Embedding(49408, 512)\n",
       "            (position_embedding): Embedding(77, 512)\n",
       "          )\n",
       "          (encoder): CLIPEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (1): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (2): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (3): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (4): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (5): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (6): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (7): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (8): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (9): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (10): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (11): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (neck): YOLOWorldPAFPN(\n",
       "    (reduce_layers): ModuleList(\n",
       "      (0): Identity()\n",
       "      (1): Identity()\n",
       "      (2): Identity()\n",
       "    )\n",
       "    (upsample_layers): ModuleList(\n",
       "      (0): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (1): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    )\n",
       "    (top_down_layers): ModuleList(\n",
       "      (0): MaxSigmoidCSPLayerWithTwoConv(\n",
       "        (main_conv): ConvModule(\n",
       "          (conv): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (activate): SiLU(inplace=True)\n",
       "        )\n",
       "        (final_conv): ConvModule(\n",
       "          (conv): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (activate): SiLU(inplace=True)\n",
       "        )\n",
       "        (blocks): ModuleList(\n",
       "          (0): DarknetBottleneck(\n",
       "            (conv1): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "            (conv2): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (1): DarknetBottleneck(\n",
       "            (conv1): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "            (conv2): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (2): DarknetBottleneck(\n",
       "            (conv1): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "            (conv2): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (attn_block): MaxSigmoidAttnBlock(\n",
       "          (guide_fc): Linear(in_features=512, out_features=320, bias=True)\n",
       "          (project_conv): ConvModule(\n",
       "            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): MaxSigmoidCSPLayerWithTwoConv(\n",
       "        (main_conv): ConvModule(\n",
       "          (conv): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (activate): SiLU(inplace=True)\n",
       "        )\n",
       "        (final_conv): ConvModule(\n",
       "          (conv): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (activate): SiLU(inplace=True)\n",
       "        )\n",
       "        (blocks): ModuleList(\n",
       "          (0): DarknetBottleneck(\n",
       "            (conv1): ConvModule(\n",
       "              (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "            (conv2): ConvModule(\n",
       "              (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (1): DarknetBottleneck(\n",
       "            (conv1): ConvModule(\n",
       "              (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "            (conv2): ConvModule(\n",
       "              (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (2): DarknetBottleneck(\n",
       "            (conv1): ConvModule(\n",
       "              (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "            (conv2): ConvModule(\n",
       "              (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (attn_block): MaxSigmoidAttnBlock(\n",
       "          (guide_fc): Linear(in_features=512, out_features=160, bias=True)\n",
       "          (project_conv): ConvModule(\n",
       "            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (downsample_layers): ModuleList(\n",
       "      (0): ConvModule(\n",
       "        (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (activate): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): ConvModule(\n",
       "        (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (activate): SiLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (bottom_up_layers): ModuleList(\n",
       "      (0): MaxSigmoidCSPLayerWithTwoConv(\n",
       "        (main_conv): ConvModule(\n",
       "          (conv): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (activate): SiLU(inplace=True)\n",
       "        )\n",
       "        (final_conv): ConvModule(\n",
       "          (conv): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (activate): SiLU(inplace=True)\n",
       "        )\n",
       "        (blocks): ModuleList(\n",
       "          (0): DarknetBottleneck(\n",
       "            (conv1): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "            (conv2): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (1): DarknetBottleneck(\n",
       "            (conv1): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "            (conv2): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (2): DarknetBottleneck(\n",
       "            (conv1): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "            (conv2): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (attn_block): MaxSigmoidAttnBlock(\n",
       "          (guide_fc): Linear(in_features=512, out_features=320, bias=True)\n",
       "          (project_conv): ConvModule(\n",
       "            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): MaxSigmoidCSPLayerWithTwoConv(\n",
       "        (main_conv): ConvModule(\n",
       "          (conv): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (activate): SiLU(inplace=True)\n",
       "        )\n",
       "        (final_conv): ConvModule(\n",
       "          (conv): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (activate): SiLU(inplace=True)\n",
       "        )\n",
       "        (blocks): ModuleList(\n",
       "          (0): DarknetBottleneck(\n",
       "            (conv1): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "            (conv2): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (1): DarknetBottleneck(\n",
       "            (conv1): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "            (conv2): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (2): DarknetBottleneck(\n",
       "            (conv1): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "            (conv2): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (attn_block): MaxSigmoidAttnBlock(\n",
       "          (guide_fc): Linear(in_features=512, out_features=320, bias=True)\n",
       "          (project_conv): ConvModule(\n",
       "            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (out_layers): ModuleList(\n",
       "      (0): Identity()\n",
       "      (1): Identity()\n",
       "      (2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (bbox_head): YOLOWorldHead(\n",
       "    (head_module): YOLOWorldHeadModule(\n",
       "      (cls_preds): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvModule(\n",
       "            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(320, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvModule(\n",
       "            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(320, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvModule(\n",
       "            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(320, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (reg_preds): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(320, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvModule(\n",
       "            (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(80, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(640, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvModule(\n",
       "            (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(80, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(640, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvModule(\n",
       "            (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(80, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (cls_contrasts): ModuleList(\n",
       "        (0): BNContrastiveHead(\n",
       "          (norm): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BNContrastiveHead(\n",
       "          (norm): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BNContrastiveHead(\n",
       "          (norm): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (loss_cls): CrossEntropyLoss(avg_non_ignore=False)\n",
       "    (loss_bbox): IoULoss()\n",
       "    (loss_obj): None\n",
       "    (assigner): BatchTaskAlignedAssigner()\n",
       "    (loss_dfl): DistributionFocalLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = Config.fromfile(\n",
    "        \"YOLO-World/configs/pretrain/yolo_world_v2_x_vlpan_bn_2e-3_100e_4x8gpus_obj365v1_goldg_train_1280ft_lvis_minival.py\"\n",
    "    )\n",
    "cfg.work_dir = \".\"\n",
    "cfg.load_from = \"yolo_world_v2_x_obj365v1_goldg_cc3mlite_pretrain_1280ft-14996a36.pth\"\n",
    "runner = Runner.from_cfg(cfg)\n",
    "runner.call_hook(\"before_run\")\n",
    "runner.load_or_resume()\n",
    "pipeline = cfg.test_dataloader.dataset.pipeline\n",
    "runner.pipeline = Compose(pipeline)\n",
    "runner.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c67e9f5-37b9-402e-bdf5-b4d90e004b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_yoloworld(video_path, text_query):\n",
    "    frame_count = 0\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = torch.from_numpy(frame).float().to(\"cuda\")\n",
    "        frame_count += 1\n",
    "    cap.release()\n",
    "    return frame_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6153e102-0337-494f-a76c-3a59a535020a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_yoloworld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhong_kong_airport_demo_data.mp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMcDonalds Logo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m, in \u001b[0;36mrun_yoloworld\u001b[0;34m(video_path, text_query)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m----> 9\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     frame_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     11\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.conda/envs/seiden/lib/python3.10/site-packages/torch/cuda/__init__.py:216\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    220\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "run_yoloworld('hong_kong_airport_demo_data.mp4', 'McDonalds Logo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dc43d4-fd57-4f84-9260-dfbc8088539b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
