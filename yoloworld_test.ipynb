{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0718a4c-84f8-4056-b7a2-663f1f21c02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from mmengine.config import Config\n",
    "from mmengine.dataset import Compose\n",
    "from mmengine.runner import Runner\n",
    "from mmengine.runner.amp import autocast\n",
    "from mmyolo.registry import RUNNERS\n",
    "from torchvision.ops import nms\n",
    "import cv2\n",
    "import time\n",
    "import torch.cuda as cuda\n",
    "from PIL import Image\n",
    "from mmdet.apis import init_detector\n",
    "from mmengine.utils import track_iter_progress\n",
    "from mmyolo.registry import VISUALIZERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3bd4569-f62a-4ad4-b718-bc5304942c26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/23 19:39:15 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmyolo\" in the \"log_processor\" registry tree. As a workaround, the current \"log_processor\" registry in \"mmengine\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmyolo\" is a correct scope, or whether the registry is initialized.\n",
      "12/23 19:39:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.10.16 (main, Dec  4 2024, 08:53:37) [GCC 9.4.0]\n",
      "    CUDA available: True\n",
      "    MUSA available: False\n",
      "    numpy_random_seed: 1678332170\n",
      "    GPU 0: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "    CUDA_HOME: /usr/local/cuda\n",
      "    NVCC: Cuda compilation tools, release 11.3, V11.3.109\n",
      "    GCC: x86_64-linux-gnu-gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\n",
      "    PyTorch: 1.11.0+cu113\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 7.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.5.2 (Git Hash a9302535553c73243c632ad3c4c80beec3d19a1e)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.3\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
      "  - CuDNN 8.2\n",
      "  - Magma 2.5.2\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.12.0+cu113\n",
      "    OpenCV: 4.10.0\n",
      "    MMEngine: 0.10.3\n",
      "\n",
      "Runtime environment:\n",
      "    cudnn_benchmark: True\n",
      "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 1678332170\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "12/23 19:39:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Config:\n",
      "_backend_args = None\n",
      "_multiscale_resize_transforms = [\n",
      "    dict(\n",
      "        transforms=[\n",
      "            dict(scale=(\n",
      "                640,\n",
      "                640,\n",
      "            ), type='YOLOv5KeepRatioResize'),\n",
      "            dict(\n",
      "                allow_scale_up=False,\n",
      "                pad_val=dict(img=114),\n",
      "                scale=(\n",
      "                    640,\n",
      "                    640,\n",
      "                ),\n",
      "                type='LetterResize'),\n",
      "        ],\n",
      "        type='Compose'),\n",
      "    dict(\n",
      "        transforms=[\n",
      "            dict(scale=(\n",
      "                320,\n",
      "                320,\n",
      "            ), type='YOLOv5KeepRatioResize'),\n",
      "            dict(\n",
      "                allow_scale_up=False,\n",
      "                pad_val=dict(img=114),\n",
      "                scale=(\n",
      "                    320,\n",
      "                    320,\n",
      "                ),\n",
      "                type='LetterResize'),\n",
      "        ],\n",
      "        type='Compose'),\n",
      "    dict(\n",
      "        transforms=[\n",
      "            dict(scale=(\n",
      "                960,\n",
      "                960,\n",
      "            ), type='YOLOv5KeepRatioResize'),\n",
      "            dict(\n",
      "                allow_scale_up=False,\n",
      "                pad_val=dict(img=114),\n",
      "                scale=(\n",
      "                    960,\n",
      "                    960,\n",
      "                ),\n",
      "                type='LetterResize'),\n",
      "        ],\n",
      "        type='Compose'),\n",
      "]\n",
      "affine_scale = 0.9\n",
      "albu_train_transforms = [\n",
      "    dict(p=0.01, type='Blur'),\n",
      "    dict(p=0.01, type='MedianBlur'),\n",
      "    dict(p=0.01, type='ToGray'),\n",
      "    dict(p=0.01, type='CLAHE'),\n",
      "]\n",
      "backend_args = None\n",
      "base_lr = 0.002\n",
      "batch_shapes_cfg = None\n",
      "close_mosaic_epochs = 2\n",
      "coco_val_dataset = dict(\n",
      "    _delete_=True,\n",
      "    class_text_path='data/texts/lvis_v1_class_texts.json',\n",
      "    dataset=dict(\n",
      "        ann_file='lvis/lvis_v1_minival_inserted_image_name.json',\n",
      "        batch_shapes_cfg=None,\n",
      "        data_prefix=dict(img=''),\n",
      "        data_root='data/coco/',\n",
      "        test_mode=True,\n",
      "        type='YOLOv5LVISV1Dataset'),\n",
      "    pipeline=[\n",
      "        dict(type='LoadImageFromFile'),\n",
      "        dict(scale=(\n",
      "            1280,\n",
      "            1280,\n",
      "        ), type='YOLOv5KeepRatioResize'),\n",
      "        dict(\n",
      "            allow_scale_up=False,\n",
      "            pad_val=dict(img=114),\n",
      "            scale=(\n",
      "                1280,\n",
      "                1280,\n",
      "            ),\n",
      "            type='LetterResize'),\n",
      "        dict(_scope_='mmdet', type='LoadAnnotations', with_bbox=True),\n",
      "        dict(type='LoadText'),\n",
      "        dict(\n",
      "            meta_keys=(\n",
      "                'img_id',\n",
      "                'img_path',\n",
      "                'ori_shape',\n",
      "                'img_shape',\n",
      "                'scale_factor',\n",
      "                'pad_param',\n",
      "                'texts',\n",
      "            ),\n",
      "            type='mmdet.PackDetInputs'),\n",
      "    ],\n",
      "    type='MultiModalDataset')\n",
      "custom_hooks = [\n",
      "    dict(\n",
      "        ema_type='ExpMomentumEMA',\n",
      "        momentum=0.0001,\n",
      "        priority=49,\n",
      "        strict_load=False,\n",
      "        type='EMAHook',\n",
      "        update_buffers=True),\n",
      "    dict(\n",
      "        switch_epoch=98,\n",
      "        switch_pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(scale=(\n",
      "                1280,\n",
      "                1280,\n",
      "            ), type='YOLOv5KeepRatioResize'),\n",
      "            dict(\n",
      "                allow_scale_up=True,\n",
      "                pad_val=dict(img=114.0),\n",
      "                scale=(\n",
      "                    1280,\n",
      "                    1280,\n",
      "                ),\n",
      "                type='LetterResize'),\n",
      "            dict(\n",
      "                border_val=(\n",
      "                    114,\n",
      "                    114,\n",
      "                    114,\n",
      "                ),\n",
      "                max_aspect_ratio=100,\n",
      "                max_rotate_degree=0.0,\n",
      "                max_shear_degree=0.0,\n",
      "                scaling_ratio_range=(\n",
      "                    0.09999999999999998,\n",
      "                    1.9,\n",
      "                ),\n",
      "                type='YOLOv5RandomAffine'),\n",
      "            dict(\n",
      "                bbox_params=dict(\n",
      "                    format='pascal_voc',\n",
      "                    label_fields=[\n",
      "                        'gt_bboxes_labels',\n",
      "                        'gt_ignore_flags',\n",
      "                    ],\n",
      "                    type='BboxParams'),\n",
      "                keymap=dict(gt_bboxes='bboxes', img='image'),\n",
      "                transforms=[\n",
      "                    dict(p=0.01, type='Blur'),\n",
      "                    dict(p=0.01, type='MedianBlur'),\n",
      "                    dict(p=0.01, type='ToGray'),\n",
      "                    dict(p=0.01, type='CLAHE'),\n",
      "                ],\n",
      "                type='mmdet.Albu'),\n",
      "            dict(type='YOLOv5HSVRandomAug'),\n",
      "            dict(prob=0.5, type='mmdet.RandomFlip'),\n",
      "            dict(\n",
      "                max_num_samples=80,\n",
      "                num_neg_samples=(\n",
      "                    1203,\n",
      "                    1203,\n",
      "                ),\n",
      "                padding_to_max=True,\n",
      "                padding_value='',\n",
      "                type='RandomLoadText'),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'flip',\n",
      "                    'flip_direction',\n",
      "                    'texts',\n",
      "                ),\n",
      "                type='mmdet.PackDetInputs'),\n",
      "        ],\n",
      "        type='mmdet.PipelineSwitchHook'),\n",
      "]\n",
      "custom_imports = dict(\n",
      "    allow_failed_imports=False, imports=[\n",
      "        'yolo_world',\n",
      "    ])\n",
      "data_root = 'data/coco/'\n",
      "dataset_type = 'YOLOv5CocoDataset'\n",
      "deepen_factor = 1.0\n",
      "default_hooks = dict(\n",
      "    checkpoint=dict(\n",
      "        interval=2,\n",
      "        max_keep_ckpts=2,\n",
      "        rule='greater',\n",
      "        save_best='auto',\n",
      "        type='CheckpointHook'),\n",
      "    logger=dict(interval=50, type='LoggerHook'),\n",
      "    param_scheduler=dict(\n",
      "        lr_factor=0.01,\n",
      "        max_epochs=100,\n",
      "        scheduler_type='linear',\n",
      "        type='YOLOv5ParamSchedulerHook'),\n",
      "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
      "    timer=dict(type='IterTimerHook'),\n",
      "    visualization=dict(type='mmdet.DetVisualizationHook'))\n",
      "default_scope = 'mmyolo'\n",
      "env_cfg = dict(\n",
      "    cudnn_benchmark=True,\n",
      "    dist_cfg=dict(backend='nccl'),\n",
      "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
      "flickr_train_dataset = dict(\n",
      "    ann_file='annotations/final_flickr_separateGT_train.json',\n",
      "    data_prefix=dict(img='full_images/'),\n",
      "    data_root='data/flickr/',\n",
      "    filter_cfg=dict(filter_empty_gt=True, min_size=32),\n",
      "    pipeline=[\n",
      "        dict(backend_args=None, type='LoadImageFromFile'),\n",
      "        dict(type='LoadAnnotations', with_bbox=True),\n",
      "        dict(\n",
      "            img_scale=(\n",
      "                1280,\n",
      "                1280,\n",
      "            ),\n",
      "            pad_val=114.0,\n",
      "            pre_transform=[\n",
      "                dict(backend_args=None, type='LoadImageFromFile'),\n",
      "                dict(type='LoadAnnotations', with_bbox=True),\n",
      "            ],\n",
      "            type='MultiModalMosaic'),\n",
      "        dict(\n",
      "            border=(\n",
      "                -640,\n",
      "                -640,\n",
      "            ),\n",
      "            border_val=(\n",
      "                114,\n",
      "                114,\n",
      "                114,\n",
      "            ),\n",
      "            max_aspect_ratio=100,\n",
      "            max_rotate_degree=0.0,\n",
      "            max_shear_degree=0.0,\n",
      "            scaling_ratio_range=(\n",
      "                0.09999999999999998,\n",
      "                1.9,\n",
      "            ),\n",
      "            type='YOLOv5RandomAffine'),\n",
      "        dict(\n",
      "            bbox_params=dict(\n",
      "                format='pascal_voc',\n",
      "                label_fields=[\n",
      "                    'gt_bboxes_labels',\n",
      "                    'gt_ignore_flags',\n",
      "                ],\n",
      "                type='BboxParams'),\n",
      "            keymap=dict(gt_bboxes='bboxes', img='image'),\n",
      "            transforms=[\n",
      "                dict(p=0.01, type='Blur'),\n",
      "                dict(p=0.01, type='MedianBlur'),\n",
      "                dict(p=0.01, type='ToGray'),\n",
      "                dict(p=0.01, type='CLAHE'),\n",
      "            ],\n",
      "            type='mmdet.Albu'),\n",
      "        dict(type='YOLOv5HSVRandomAug'),\n",
      "        dict(prob=0.5, type='mmdet.RandomFlip'),\n",
      "        dict(\n",
      "            max_num_samples=80,\n",
      "            num_neg_samples=(\n",
      "                1203,\n",
      "                1203,\n",
      "            ),\n",
      "            padding_to_max=True,\n",
      "            padding_value='',\n",
      "            type='RandomLoadText'),\n",
      "        dict(\n",
      "            meta_keys=(\n",
      "                'img_id',\n",
      "                'img_path',\n",
      "                'ori_shape',\n",
      "                'img_shape',\n",
      "                'flip',\n",
      "                'flip_direction',\n",
      "                'texts',\n",
      "            ),\n",
      "            type='mmdet.PackDetInputs'),\n",
      "    ],\n",
      "    type='YOLOv5MixedGroundingDataset')\n",
      "img_scale = (\n",
      "    1280,\n",
      "    1280,\n",
      ")\n",
      "img_scales = [\n",
      "    (\n",
      "        640,\n",
      "        640,\n",
      "    ),\n",
      "    (\n",
      "        320,\n",
      "        320,\n",
      "    ),\n",
      "    (\n",
      "        960,\n",
      "        960,\n",
      "    ),\n",
      "]\n",
      "last_stage_out_channels = 512\n",
      "last_transform = [\n",
      "    dict(\n",
      "        bbox_params=dict(\n",
      "            format='pascal_voc',\n",
      "            label_fields=[\n",
      "                'gt_bboxes_labels',\n",
      "                'gt_ignore_flags',\n",
      "            ],\n",
      "            type='BboxParams'),\n",
      "        keymap=dict(gt_bboxes='bboxes', img='image'),\n",
      "        transforms=[\n",
      "            dict(p=0.01, type='Blur'),\n",
      "            dict(p=0.01, type='MedianBlur'),\n",
      "            dict(p=0.01, type='ToGray'),\n",
      "            dict(p=0.01, type='CLAHE'),\n",
      "        ],\n",
      "        type='mmdet.Albu'),\n",
      "    dict(type='YOLOv5HSVRandomAug'),\n",
      "    dict(prob=0.5, type='mmdet.RandomFlip'),\n",
      "    dict(\n",
      "        meta_keys=(\n",
      "            'img_id',\n",
      "            'img_path',\n",
      "            'ori_shape',\n",
      "            'img_shape',\n",
      "            'flip',\n",
      "            'flip_direction',\n",
      "        ),\n",
      "        type='mmdet.PackDetInputs'),\n",
      "]\n",
      "load_from = 'yolo_world_v2_x_obj365v1_goldg_cc3mlite_pretrain_1280ft-14996a36.pth'\n",
      "log_level = 'INFO'\n",
      "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)\n",
      "loss_bbox_weight = 7.5\n",
      "loss_cls_weight = 0.5\n",
      "loss_dfl_weight = 0.375\n",
      "lr_factor = 0.01\n",
      "max_aspect_ratio = 100\n",
      "max_epochs = 100\n",
      "max_keep_ckpts = 2\n",
      "mg_train_dataset = dict(\n",
      "    ann_file='annotations/final_mixed_train_no_coco.json',\n",
      "    data_prefix=dict(img='gqa/images/'),\n",
      "    data_root='data/mixed_grounding/',\n",
      "    filter_cfg=dict(filter_empty_gt=False, min_size=32),\n",
      "    pipeline=[\n",
      "        dict(backend_args=None, type='LoadImageFromFile'),\n",
      "        dict(type='LoadAnnotations', with_bbox=True),\n",
      "        dict(\n",
      "            img_scale=(\n",
      "                1280,\n",
      "                1280,\n",
      "            ),\n",
      "            pad_val=114.0,\n",
      "            pre_transform=[\n",
      "                dict(backend_args=None, type='LoadImageFromFile'),\n",
      "                dict(type='LoadAnnotations', with_bbox=True),\n",
      "            ],\n",
      "            type='MultiModalMosaic'),\n",
      "        dict(\n",
      "            border=(\n",
      "                -640,\n",
      "                -640,\n",
      "            ),\n",
      "            border_val=(\n",
      "                114,\n",
      "                114,\n",
      "                114,\n",
      "            ),\n",
      "            max_aspect_ratio=100,\n",
      "            max_rotate_degree=0.0,\n",
      "            max_shear_degree=0.0,\n",
      "            scaling_ratio_range=(\n",
      "                0.09999999999999998,\n",
      "                1.9,\n",
      "            ),\n",
      "            type='YOLOv5RandomAffine'),\n",
      "        dict(\n",
      "            bbox_params=dict(\n",
      "                format='pascal_voc',\n",
      "                label_fields=[\n",
      "                    'gt_bboxes_labels',\n",
      "                    'gt_ignore_flags',\n",
      "                ],\n",
      "                type='BboxParams'),\n",
      "            keymap=dict(gt_bboxes='bboxes', img='image'),\n",
      "            transforms=[\n",
      "                dict(p=0.01, type='Blur'),\n",
      "                dict(p=0.01, type='MedianBlur'),\n",
      "                dict(p=0.01, type='ToGray'),\n",
      "                dict(p=0.01, type='CLAHE'),\n",
      "            ],\n",
      "            type='mmdet.Albu'),\n",
      "        dict(type='YOLOv5HSVRandomAug'),\n",
      "        dict(prob=0.5, type='mmdet.RandomFlip'),\n",
      "        dict(\n",
      "            max_num_samples=80,\n",
      "            num_neg_samples=(\n",
      "                1203,\n",
      "                1203,\n",
      "            ),\n",
      "            padding_to_max=True,\n",
      "            padding_value='',\n",
      "            type='RandomLoadText'),\n",
      "        dict(\n",
      "            meta_keys=(\n",
      "                'img_id',\n",
      "                'img_path',\n",
      "                'ori_shape',\n",
      "                'img_shape',\n",
      "                'flip',\n",
      "                'flip_direction',\n",
      "                'texts',\n",
      "            ),\n",
      "            type='mmdet.PackDetInputs'),\n",
      "    ],\n",
      "    type='YOLOv5MixedGroundingDataset')\n",
      "mixup_prob = 0.15\n",
      "model = dict(\n",
      "    backbone=dict(\n",
      "        image_model=dict(\n",
      "            act_cfg=dict(inplace=True, type='SiLU'),\n",
      "            arch='P5',\n",
      "            deepen_factor=1.0,\n",
      "            last_stage_out_channels=512,\n",
      "            norm_cfg=dict(eps=0.001, momentum=0.03, type='BN'),\n",
      "            type='YOLOv8CSPDarknet',\n",
      "            widen_factor=1.25),\n",
      "        text_model=dict(\n",
      "            frozen_modules=[\n",
      "                'all',\n",
      "            ],\n",
      "            model_name='openai/clip-vit-base-patch32',\n",
      "            type='HuggingCLIPLanguageBackbone'),\n",
      "        type='MultiModalYOLOBackbone'),\n",
      "    bbox_head=dict(\n",
      "        bbox_coder=dict(type='DistancePointBBoxCoder'),\n",
      "        head_module=dict(\n",
      "            act_cfg=dict(inplace=True, type='SiLU'),\n",
      "            embed_dims=512,\n",
      "            featmap_strides=[\n",
      "                8,\n",
      "                16,\n",
      "                32,\n",
      "            ],\n",
      "            in_channels=[\n",
      "                256,\n",
      "                512,\n",
      "                512,\n",
      "            ],\n",
      "            norm_cfg=dict(eps=0.001, momentum=0.03, type='BN'),\n",
      "            num_classes=80,\n",
      "            reg_max=16,\n",
      "            type='YOLOWorldHeadModule',\n",
      "            use_bn_head=True,\n",
      "            widen_factor=1.25),\n",
      "        loss_bbox=dict(\n",
      "            bbox_format='xyxy',\n",
      "            iou_mode='ciou',\n",
      "            loss_weight=7.5,\n",
      "            reduction='sum',\n",
      "            return_iou=False,\n",
      "            type='IoULoss'),\n",
      "        loss_cls=dict(\n",
      "            loss_weight=0.5,\n",
      "            reduction='none',\n",
      "            type='mmdet.CrossEntropyLoss',\n",
      "            use_sigmoid=True),\n",
      "        loss_dfl=dict(\n",
      "            loss_weight=0.375,\n",
      "            reduction='mean',\n",
      "            type='mmdet.DistributionFocalLoss'),\n",
      "        prior_generator=dict(\n",
      "            offset=0.5, strides=[\n",
      "                8,\n",
      "                16,\n",
      "                32,\n",
      "            ], type='mmdet.MlvlPointGenerator'),\n",
      "        type='YOLOWorldHead'),\n",
      "    data_preprocessor=dict(\n",
      "        bgr_to_rgb=True,\n",
      "        mean=[\n",
      "            0.0,\n",
      "            0.0,\n",
      "            0.0,\n",
      "        ],\n",
      "        std=[\n",
      "            255.0,\n",
      "            255.0,\n",
      "            255.0,\n",
      "        ],\n",
      "        type='YOLOWDetDataPreprocessor'),\n",
      "    mm_neck=True,\n",
      "    neck=dict(\n",
      "        act_cfg=dict(inplace=True, type='SiLU'),\n",
      "        block_cfg=dict(type='MaxSigmoidCSPLayerWithTwoConv'),\n",
      "        deepen_factor=1.0,\n",
      "        embed_channels=[\n",
      "            128,\n",
      "            256,\n",
      "            256,\n",
      "        ],\n",
      "        guide_channels=512,\n",
      "        in_channels=[\n",
      "            256,\n",
      "            512,\n",
      "            512,\n",
      "        ],\n",
      "        norm_cfg=dict(eps=0.001, momentum=0.03, type='BN'),\n",
      "        num_csp_blocks=3,\n",
      "        num_heads=[\n",
      "            4,\n",
      "            8,\n",
      "            8,\n",
      "        ],\n",
      "        out_channels=[\n",
      "            256,\n",
      "            512,\n",
      "            512,\n",
      "        ],\n",
      "        type='YOLOWorldPAFPN',\n",
      "        widen_factor=1.25),\n",
      "    num_test_classes=1203,\n",
      "    num_train_classes=80,\n",
      "    test_cfg=dict(\n",
      "        max_per_img=300,\n",
      "        multi_label=True,\n",
      "        nms=dict(iou_threshold=0.7, type='nms'),\n",
      "        nms_pre=30000,\n",
      "        score_thr=0.001),\n",
      "    train_cfg=dict(\n",
      "        assigner=dict(\n",
      "            alpha=0.5,\n",
      "            beta=6.0,\n",
      "            eps=1e-09,\n",
      "            num_classes=80,\n",
      "            topk=10,\n",
      "            type='BatchTaskAlignedAssigner',\n",
      "            use_ciou=True)),\n",
      "    type='YOLOWorldDetector')\n",
      "model_test_cfg = dict(\n",
      "    max_per_img=300,\n",
      "    multi_label=True,\n",
      "    nms=dict(iou_threshold=0.7, type='nms'),\n",
      "    nms_pre=30000,\n",
      "    score_thr=0.001)\n",
      "mosaic_affine_transform = [\n",
      "    dict(\n",
      "        img_scale=(\n",
      "            640,\n",
      "            640,\n",
      "        ),\n",
      "        pad_val=114.0,\n",
      "        pre_transform=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "        ],\n",
      "        type='Mosaic'),\n",
      "    dict(\n",
      "        border=(\n",
      "            -320,\n",
      "            -320,\n",
      "        ),\n",
      "        border_val=(\n",
      "            114,\n",
      "            114,\n",
      "            114,\n",
      "        ),\n",
      "        max_aspect_ratio=100,\n",
      "        max_rotate_degree=0.0,\n",
      "        max_shear_degree=0.0,\n",
      "        scaling_ratio_range=(\n",
      "            0.09999999999999998,\n",
      "            1.9,\n",
      "        ),\n",
      "        type='YOLOv5RandomAffine'),\n",
      "]\n",
      "neck_embed_channels = [\n",
      "    128,\n",
      "    256,\n",
      "    256,\n",
      "]\n",
      "neck_num_heads = [\n",
      "    4,\n",
      "    8,\n",
      "    8,\n",
      "]\n",
      "norm_cfg = dict(eps=0.001, momentum=0.03, type='BN')\n",
      "num_classes = 1203\n",
      "num_det_layers = 3\n",
      "num_training_classes = 80\n",
      "obj365v1_train_dataset = dict(\n",
      "    class_text_path='data/texts/obj365v1_class_texts.json',\n",
      "    dataset=dict(\n",
      "        ann_file='annotations/objects365_train.json',\n",
      "        data_prefix=dict(img='train/'),\n",
      "        data_root='data/objects365v1/',\n",
      "        filter_cfg=dict(filter_empty_gt=False, min_size=32),\n",
      "        type='YOLOv5Objects365V1Dataset'),\n",
      "    pipeline=[\n",
      "        dict(backend_args=None, type='LoadImageFromFile'),\n",
      "        dict(type='LoadAnnotations', with_bbox=True),\n",
      "        dict(\n",
      "            img_scale=(\n",
      "                1280,\n",
      "                1280,\n",
      "            ),\n",
      "            pad_val=114.0,\n",
      "            pre_transform=[\n",
      "                dict(backend_args=None, type='LoadImageFromFile'),\n",
      "                dict(type='LoadAnnotations', with_bbox=True),\n",
      "            ],\n",
      "            type='MultiModalMosaic'),\n",
      "        dict(\n",
      "            border=(\n",
      "                -640,\n",
      "                -640,\n",
      "            ),\n",
      "            border_val=(\n",
      "                114,\n",
      "                114,\n",
      "                114,\n",
      "            ),\n",
      "            max_aspect_ratio=100,\n",
      "            max_rotate_degree=0.0,\n",
      "            max_shear_degree=0.0,\n",
      "            scaling_ratio_range=(\n",
      "                0.09999999999999998,\n",
      "                1.9,\n",
      "            ),\n",
      "            type='YOLOv5RandomAffine'),\n",
      "        dict(\n",
      "            bbox_params=dict(\n",
      "                format='pascal_voc',\n",
      "                label_fields=[\n",
      "                    'gt_bboxes_labels',\n",
      "                    'gt_ignore_flags',\n",
      "                ],\n",
      "                type='BboxParams'),\n",
      "            keymap=dict(gt_bboxes='bboxes', img='image'),\n",
      "            transforms=[\n",
      "                dict(p=0.01, type='Blur'),\n",
      "                dict(p=0.01, type='MedianBlur'),\n",
      "                dict(p=0.01, type='ToGray'),\n",
      "                dict(p=0.01, type='CLAHE'),\n",
      "            ],\n",
      "            type='mmdet.Albu'),\n",
      "        dict(type='YOLOv5HSVRandomAug'),\n",
      "        dict(prob=0.5, type='mmdet.RandomFlip'),\n",
      "        dict(\n",
      "            max_num_samples=80,\n",
      "            num_neg_samples=(\n",
      "                1203,\n",
      "                1203,\n",
      "            ),\n",
      "            padding_to_max=True,\n",
      "            padding_value='',\n",
      "            type='RandomLoadText'),\n",
      "        dict(\n",
      "            meta_keys=(\n",
      "                'img_id',\n",
      "                'img_path',\n",
      "                'ori_shape',\n",
      "                'img_shape',\n",
      "                'flip',\n",
      "                'flip_direction',\n",
      "                'texts',\n",
      "            ),\n",
      "            type='mmdet.PackDetInputs'),\n",
      "    ],\n",
      "    type='MultiModalDataset')\n",
      "optim_wrapper = dict(\n",
      "    clip_grad=dict(max_norm=10.0),\n",
      "    constructor='YOLOWv5OptimizerConstructor',\n",
      "    optimizer=dict(\n",
      "        batch_size_per_gpu=16, lr=0.002, type='AdamW', weight_decay=0.025),\n",
      "    paramwise_cfg=dict(\n",
      "        bias_decay_mult=0.0,\n",
      "        custom_keys=dict({\n",
      "            'backbone.text_model': dict(lr_mult=0.01),\n",
      "            'logit_scale': dict(weight_decay=0.0)\n",
      "        }),\n",
      "        norm_decay_mult=0.0),\n",
      "    type='OptimWrapper')\n",
      "param_scheduler = None\n",
      "persistent_workers = True\n",
      "pre_transform = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "]\n",
      "resume = False\n",
      "save_epoch_intervals = 2\n",
      "strides = [\n",
      "    8,\n",
      "    16,\n",
      "    32,\n",
      "]\n",
      "tal_alpha = 0.5\n",
      "tal_beta = 6.0\n",
      "tal_topk = 10\n",
      "test_cfg = dict(type='TestLoop')\n",
      "test_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        class_text_path='data/texts/lvis_v1_class_texts.json',\n",
      "        dataset=dict(\n",
      "            ann_file='lvis/lvis_v1_minival_inserted_image_name.json',\n",
      "            batch_shapes_cfg=None,\n",
      "            data_prefix=dict(img=''),\n",
      "            data_root='data/coco/',\n",
      "            test_mode=True,\n",
      "            type='YOLOv5LVISV1Dataset'),\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(scale=(\n",
      "                1280,\n",
      "                1280,\n",
      "            ), type='YOLOv5KeepRatioResize'),\n",
      "            dict(\n",
      "                allow_scale_up=False,\n",
      "                pad_val=dict(img=114),\n",
      "                scale=(\n",
      "                    1280,\n",
      "                    1280,\n",
      "                ),\n",
      "                type='LetterResize'),\n",
      "            dict(_scope_='mmdet', type='LoadAnnotations', with_bbox=True),\n",
      "            dict(type='LoadText'),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'scale_factor',\n",
      "                    'pad_param',\n",
      "                    'texts',\n",
      "                ),\n",
      "                type='mmdet.PackDetInputs'),\n",
      "        ],\n",
      "        type='MultiModalDataset'),\n",
      "    drop_last=False,\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    pin_memory=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "test_evaluator = dict(\n",
      "    ann_file='data/coco/lvis/lvis_v1_minival_inserted_image_name.json',\n",
      "    metric='bbox',\n",
      "    proposal_nums=(\n",
      "        100,\n",
      "        1,\n",
      "        10,\n",
      "    ),\n",
      "    type='mmdet.LVISMetric')\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(scale=(\n",
      "        1280,\n",
      "        1280,\n",
      "    ), type='YOLOv5KeepRatioResize'),\n",
      "    dict(\n",
      "        allow_scale_up=False,\n",
      "        pad_val=dict(img=114),\n",
      "        scale=(\n",
      "            1280,\n",
      "            1280,\n",
      "        ),\n",
      "        type='LetterResize'),\n",
      "    dict(_scope_='mmdet', type='LoadAnnotations', with_bbox=True),\n",
      "    dict(type='LoadText'),\n",
      "    dict(\n",
      "        meta_keys=(\n",
      "            'img_id',\n",
      "            'img_path',\n",
      "            'ori_shape',\n",
      "            'img_shape',\n",
      "            'scale_factor',\n",
      "            'pad_param',\n",
      "            'texts',\n",
      "        ),\n",
      "        type='mmdet.PackDetInputs'),\n",
      "]\n",
      "text_channels = 512\n",
      "text_model_name = 'openai/clip-vit-base-patch32'\n",
      "text_transform = [\n",
      "    dict(\n",
      "        max_num_samples=80,\n",
      "        num_neg_samples=(\n",
      "            1203,\n",
      "            1203,\n",
      "        ),\n",
      "        padding_to_max=True,\n",
      "        padding_value='',\n",
      "        type='RandomLoadText'),\n",
      "    dict(\n",
      "        meta_keys=(\n",
      "            'img_id',\n",
      "            'img_path',\n",
      "            'ori_shape',\n",
      "            'img_shape',\n",
      "            'flip',\n",
      "            'flip_direction',\n",
      "            'texts',\n",
      "        ),\n",
      "        type='mmdet.PackDetInputs'),\n",
      "]\n",
      "train_ann_file = 'annotations/instances_train2017.json'\n",
      "train_batch_size_per_gpu = 16\n",
      "train_cfg = dict(\n",
      "    dynamic_intervals=[\n",
      "        (\n",
      "            98,\n",
      "            1,\n",
      "        ),\n",
      "    ],\n",
      "    max_epochs=100,\n",
      "    type='EpochBasedTrainLoop',\n",
      "    val_interval=10)\n",
      "train_data_prefix = 'train2017/'\n",
      "train_dataloader = dict(\n",
      "    batch_size=16,\n",
      "    collate_fn=dict(type='yolow_collate'),\n",
      "    dataset=dict(\n",
      "        datasets=[\n",
      "            dict(\n",
      "                class_text_path='data/texts/obj365v1_class_texts.json',\n",
      "                dataset=dict(\n",
      "                    ann_file='annotations/objects365_train.json',\n",
      "                    data_prefix=dict(img='train/'),\n",
      "                    data_root='data/objects365v1/',\n",
      "                    filter_cfg=dict(filter_empty_gt=False, min_size=32),\n",
      "                    type='YOLOv5Objects365V1Dataset'),\n",
      "                pipeline=[\n",
      "                    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "                    dict(type='LoadAnnotations', with_bbox=True),\n",
      "                    dict(\n",
      "                        img_scale=(\n",
      "                            1280,\n",
      "                            1280,\n",
      "                        ),\n",
      "                        pad_val=114.0,\n",
      "                        pre_transform=[\n",
      "                            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "                            dict(type='LoadAnnotations', with_bbox=True),\n",
      "                        ],\n",
      "                        type='MultiModalMosaic'),\n",
      "                    dict(\n",
      "                        border=(\n",
      "                            -640,\n",
      "                            -640,\n",
      "                        ),\n",
      "                        border_val=(\n",
      "                            114,\n",
      "                            114,\n",
      "                            114,\n",
      "                        ),\n",
      "                        max_aspect_ratio=100,\n",
      "                        max_rotate_degree=0.0,\n",
      "                        max_shear_degree=0.0,\n",
      "                        scaling_ratio_range=(\n",
      "                            0.09999999999999998,\n",
      "                            1.9,\n",
      "                        ),\n",
      "                        type='YOLOv5RandomAffine'),\n",
      "                    dict(\n",
      "                        bbox_params=dict(\n",
      "                            format='pascal_voc',\n",
      "                            label_fields=[\n",
      "                                'gt_bboxes_labels',\n",
      "                                'gt_ignore_flags',\n",
      "                            ],\n",
      "                            type='BboxParams'),\n",
      "                        keymap=dict(gt_bboxes='bboxes', img='image'),\n",
      "                        transforms=[\n",
      "                            dict(p=0.01, type='Blur'),\n",
      "                            dict(p=0.01, type='MedianBlur'),\n",
      "                            dict(p=0.01, type='ToGray'),\n",
      "                            dict(p=0.01, type='CLAHE'),\n",
      "                        ],\n",
      "                        type='mmdet.Albu'),\n",
      "                    dict(type='YOLOv5HSVRandomAug'),\n",
      "                    dict(prob=0.5, type='mmdet.RandomFlip'),\n",
      "                    dict(\n",
      "                        max_num_samples=80,\n",
      "                        num_neg_samples=(\n",
      "                            1203,\n",
      "                            1203,\n",
      "                        ),\n",
      "                        padding_to_max=True,\n",
      "                        padding_value='',\n",
      "                        type='RandomLoadText'),\n",
      "                    dict(\n",
      "                        meta_keys=(\n",
      "                            'img_id',\n",
      "                            'img_path',\n",
      "                            'ori_shape',\n",
      "                            'img_shape',\n",
      "                            'flip',\n",
      "                            'flip_direction',\n",
      "                            'texts',\n",
      "                        ),\n",
      "                        type='mmdet.PackDetInputs'),\n",
      "                ],\n",
      "                type='MultiModalDataset'),\n",
      "            dict(\n",
      "                ann_file='annotations/final_flickr_separateGT_train.json',\n",
      "                data_prefix=dict(img='full_images/'),\n",
      "                data_root='data/flickr/',\n",
      "                filter_cfg=dict(filter_empty_gt=True, min_size=32),\n",
      "                pipeline=[\n",
      "                    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "                    dict(type='LoadAnnotations', with_bbox=True),\n",
      "                    dict(\n",
      "                        img_scale=(\n",
      "                            1280,\n",
      "                            1280,\n",
      "                        ),\n",
      "                        pad_val=114.0,\n",
      "                        pre_transform=[\n",
      "                            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "                            dict(type='LoadAnnotations', with_bbox=True),\n",
      "                        ],\n",
      "                        type='MultiModalMosaic'),\n",
      "                    dict(\n",
      "                        border=(\n",
      "                            -640,\n",
      "                            -640,\n",
      "                        ),\n",
      "                        border_val=(\n",
      "                            114,\n",
      "                            114,\n",
      "                            114,\n",
      "                        ),\n",
      "                        max_aspect_ratio=100,\n",
      "                        max_rotate_degree=0.0,\n",
      "                        max_shear_degree=0.0,\n",
      "                        scaling_ratio_range=(\n",
      "                            0.09999999999999998,\n",
      "                            1.9,\n",
      "                        ),\n",
      "                        type='YOLOv5RandomAffine'),\n",
      "                    dict(\n",
      "                        bbox_params=dict(\n",
      "                            format='pascal_voc',\n",
      "                            label_fields=[\n",
      "                                'gt_bboxes_labels',\n",
      "                                'gt_ignore_flags',\n",
      "                            ],\n",
      "                            type='BboxParams'),\n",
      "                        keymap=dict(gt_bboxes='bboxes', img='image'),\n",
      "                        transforms=[\n",
      "                            dict(p=0.01, type='Blur'),\n",
      "                            dict(p=0.01, type='MedianBlur'),\n",
      "                            dict(p=0.01, type='ToGray'),\n",
      "                            dict(p=0.01, type='CLAHE'),\n",
      "                        ],\n",
      "                        type='mmdet.Albu'),\n",
      "                    dict(type='YOLOv5HSVRandomAug'),\n",
      "                    dict(prob=0.5, type='mmdet.RandomFlip'),\n",
      "                    dict(\n",
      "                        max_num_samples=80,\n",
      "                        num_neg_samples=(\n",
      "                            1203,\n",
      "                            1203,\n",
      "                        ),\n",
      "                        padding_to_max=True,\n",
      "                        padding_value='',\n",
      "                        type='RandomLoadText'),\n",
      "                    dict(\n",
      "                        meta_keys=(\n",
      "                            'img_id',\n",
      "                            'img_path',\n",
      "                            'ori_shape',\n",
      "                            'img_shape',\n",
      "                            'flip',\n",
      "                            'flip_direction',\n",
      "                            'texts',\n",
      "                        ),\n",
      "                        type='mmdet.PackDetInputs'),\n",
      "                ],\n",
      "                type='YOLOv5MixedGroundingDataset'),\n",
      "            dict(\n",
      "                ann_file='annotations/final_mixed_train_no_coco.json',\n",
      "                data_prefix=dict(img='gqa/images/'),\n",
      "                data_root='data/mixed_grounding/',\n",
      "                filter_cfg=dict(filter_empty_gt=False, min_size=32),\n",
      "                pipeline=[\n",
      "                    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "                    dict(type='LoadAnnotations', with_bbox=True),\n",
      "                    dict(\n",
      "                        img_scale=(\n",
      "                            1280,\n",
      "                            1280,\n",
      "                        ),\n",
      "                        pad_val=114.0,\n",
      "                        pre_transform=[\n",
      "                            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "                            dict(type='LoadAnnotations', with_bbox=True),\n",
      "                        ],\n",
      "                        type='MultiModalMosaic'),\n",
      "                    dict(\n",
      "                        border=(\n",
      "                            -640,\n",
      "                            -640,\n",
      "                        ),\n",
      "                        border_val=(\n",
      "                            114,\n",
      "                            114,\n",
      "                            114,\n",
      "                        ),\n",
      "                        max_aspect_ratio=100,\n",
      "                        max_rotate_degree=0.0,\n",
      "                        max_shear_degree=0.0,\n",
      "                        scaling_ratio_range=(\n",
      "                            0.09999999999999998,\n",
      "                            1.9,\n",
      "                        ),\n",
      "                        type='YOLOv5RandomAffine'),\n",
      "                    dict(\n",
      "                        bbox_params=dict(\n",
      "                            format='pascal_voc',\n",
      "                            label_fields=[\n",
      "                                'gt_bboxes_labels',\n",
      "                                'gt_ignore_flags',\n",
      "                            ],\n",
      "                            type='BboxParams'),\n",
      "                        keymap=dict(gt_bboxes='bboxes', img='image'),\n",
      "                        transforms=[\n",
      "                            dict(p=0.01, type='Blur'),\n",
      "                            dict(p=0.01, type='MedianBlur'),\n",
      "                            dict(p=0.01, type='ToGray'),\n",
      "                            dict(p=0.01, type='CLAHE'),\n",
      "                        ],\n",
      "                        type='mmdet.Albu'),\n",
      "                    dict(type='YOLOv5HSVRandomAug'),\n",
      "                    dict(prob=0.5, type='mmdet.RandomFlip'),\n",
      "                    dict(\n",
      "                        max_num_samples=80,\n",
      "                        num_neg_samples=(\n",
      "                            1203,\n",
      "                            1203,\n",
      "                        ),\n",
      "                        padding_to_max=True,\n",
      "                        padding_value='',\n",
      "                        type='RandomLoadText'),\n",
      "                    dict(\n",
      "                        meta_keys=(\n",
      "                            'img_id',\n",
      "                            'img_path',\n",
      "                            'ori_shape',\n",
      "                            'img_shape',\n",
      "                            'flip',\n",
      "                            'flip_direction',\n",
      "                            'texts',\n",
      "                        ),\n",
      "                        type='mmdet.PackDetInputs'),\n",
      "                ],\n",
      "                type='YOLOv5MixedGroundingDataset'),\n",
      "        ],\n",
      "        ignore_keys=[\n",
      "            'classes',\n",
      "            'palette',\n",
      "        ],\n",
      "        type='ConcatDataset'),\n",
      "    num_workers=8,\n",
      "    persistent_workers=True,\n",
      "    pin_memory=True,\n",
      "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
      "train_num_workers = 8\n",
      "train_pipeline = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(\n",
      "        img_scale=(\n",
      "            1280,\n",
      "            1280,\n",
      "        ),\n",
      "        pad_val=114.0,\n",
      "        pre_transform=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "        ],\n",
      "        type='MultiModalMosaic'),\n",
      "    dict(\n",
      "        border=(\n",
      "            -640,\n",
      "            -640,\n",
      "        ),\n",
      "        border_val=(\n",
      "            114,\n",
      "            114,\n",
      "            114,\n",
      "        ),\n",
      "        max_aspect_ratio=100,\n",
      "        max_rotate_degree=0.0,\n",
      "        max_shear_degree=0.0,\n",
      "        scaling_ratio_range=(\n",
      "            0.09999999999999998,\n",
      "            1.9,\n",
      "        ),\n",
      "        type='YOLOv5RandomAffine'),\n",
      "    dict(\n",
      "        bbox_params=dict(\n",
      "            format='pascal_voc',\n",
      "            label_fields=[\n",
      "                'gt_bboxes_labels',\n",
      "                'gt_ignore_flags',\n",
      "            ],\n",
      "            type='BboxParams'),\n",
      "        keymap=dict(gt_bboxes='bboxes', img='image'),\n",
      "        transforms=[\n",
      "            dict(p=0.01, type='Blur'),\n",
      "            dict(p=0.01, type='MedianBlur'),\n",
      "            dict(p=0.01, type='ToGray'),\n",
      "            dict(p=0.01, type='CLAHE'),\n",
      "        ],\n",
      "        type='mmdet.Albu'),\n",
      "    dict(type='YOLOv5HSVRandomAug'),\n",
      "    dict(prob=0.5, type='mmdet.RandomFlip'),\n",
      "    dict(\n",
      "        max_num_samples=80,\n",
      "        num_neg_samples=(\n",
      "            1203,\n",
      "            1203,\n",
      "        ),\n",
      "        padding_to_max=True,\n",
      "        padding_value='',\n",
      "        type='RandomLoadText'),\n",
      "    dict(\n",
      "        meta_keys=(\n",
      "            'img_id',\n",
      "            'img_path',\n",
      "            'ori_shape',\n",
      "            'img_shape',\n",
      "            'flip',\n",
      "            'flip_direction',\n",
      "            'texts',\n",
      "        ),\n",
      "        type='mmdet.PackDetInputs'),\n",
      "]\n",
      "train_pipeline_stage2 = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(scale=(\n",
      "        1280,\n",
      "        1280,\n",
      "    ), type='YOLOv5KeepRatioResize'),\n",
      "    dict(\n",
      "        allow_scale_up=True,\n",
      "        pad_val=dict(img=114.0),\n",
      "        scale=(\n",
      "            1280,\n",
      "            1280,\n",
      "        ),\n",
      "        type='LetterResize'),\n",
      "    dict(\n",
      "        border_val=(\n",
      "            114,\n",
      "            114,\n",
      "            114,\n",
      "        ),\n",
      "        max_aspect_ratio=100,\n",
      "        max_rotate_degree=0.0,\n",
      "        max_shear_degree=0.0,\n",
      "        scaling_ratio_range=(\n",
      "            0.09999999999999998,\n",
      "            1.9,\n",
      "        ),\n",
      "        type='YOLOv5RandomAffine'),\n",
      "    dict(\n",
      "        bbox_params=dict(\n",
      "            format='pascal_voc',\n",
      "            label_fields=[\n",
      "                'gt_bboxes_labels',\n",
      "                'gt_ignore_flags',\n",
      "            ],\n",
      "            type='BboxParams'),\n",
      "        keymap=dict(gt_bboxes='bboxes', img='image'),\n",
      "        transforms=[\n",
      "            dict(p=0.01, type='Blur'),\n",
      "            dict(p=0.01, type='MedianBlur'),\n",
      "            dict(p=0.01, type='ToGray'),\n",
      "            dict(p=0.01, type='CLAHE'),\n",
      "        ],\n",
      "        type='mmdet.Albu'),\n",
      "    dict(type='YOLOv5HSVRandomAug'),\n",
      "    dict(prob=0.5, type='mmdet.RandomFlip'),\n",
      "    dict(\n",
      "        max_num_samples=80,\n",
      "        num_neg_samples=(\n",
      "            1203,\n",
      "            1203,\n",
      "        ),\n",
      "        padding_to_max=True,\n",
      "        padding_value='',\n",
      "        type='RandomLoadText'),\n",
      "    dict(\n",
      "        meta_keys=(\n",
      "            'img_id',\n",
      "            'img_path',\n",
      "            'ori_shape',\n",
      "            'img_shape',\n",
      "            'flip',\n",
      "            'flip_direction',\n",
      "            'texts',\n",
      "        ),\n",
      "        type='mmdet.PackDetInputs'),\n",
      "]\n",
      "tta_model = dict(\n",
      "    tta_cfg=dict(max_per_img=300, nms=dict(iou_threshold=0.65, type='nms')),\n",
      "    type='mmdet.DetTTAModel')\n",
      "tta_pipeline = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        transforms=[\n",
      "            [\n",
      "                dict(\n",
      "                    transforms=[\n",
      "                        dict(scale=(\n",
      "                            640,\n",
      "                            640,\n",
      "                        ), type='YOLOv5KeepRatioResize'),\n",
      "                        dict(\n",
      "                            allow_scale_up=False,\n",
      "                            pad_val=dict(img=114),\n",
      "                            scale=(\n",
      "                                640,\n",
      "                                640,\n",
      "                            ),\n",
      "                            type='LetterResize'),\n",
      "                    ],\n",
      "                    type='Compose'),\n",
      "                dict(\n",
      "                    transforms=[\n",
      "                        dict(scale=(\n",
      "                            320,\n",
      "                            320,\n",
      "                        ), type='YOLOv5KeepRatioResize'),\n",
      "                        dict(\n",
      "                            allow_scale_up=False,\n",
      "                            pad_val=dict(img=114),\n",
      "                            scale=(\n",
      "                                320,\n",
      "                                320,\n",
      "                            ),\n",
      "                            type='LetterResize'),\n",
      "                    ],\n",
      "                    type='Compose'),\n",
      "                dict(\n",
      "                    transforms=[\n",
      "                        dict(scale=(\n",
      "                            960,\n",
      "                            960,\n",
      "                        ), type='YOLOv5KeepRatioResize'),\n",
      "                        dict(\n",
      "                            allow_scale_up=False,\n",
      "                            pad_val=dict(img=114),\n",
      "                            scale=(\n",
      "                                960,\n",
      "                                960,\n",
      "                            ),\n",
      "                            type='LetterResize'),\n",
      "                    ],\n",
      "                    type='Compose'),\n",
      "            ],\n",
      "            [\n",
      "                dict(prob=1.0, type='mmdet.RandomFlip'),\n",
      "                dict(prob=0.0, type='mmdet.RandomFlip'),\n",
      "            ],\n",
      "            [\n",
      "                dict(type='mmdet.LoadAnnotations', with_bbox=True),\n",
      "            ],\n",
      "            [\n",
      "                dict(\n",
      "                    meta_keys=(\n",
      "                        'img_id',\n",
      "                        'img_path',\n",
      "                        'ori_shape',\n",
      "                        'img_shape',\n",
      "                        'scale_factor',\n",
      "                        'pad_param',\n",
      "                        'flip',\n",
      "                        'flip_direction',\n",
      "                    ),\n",
      "                    type='mmdet.PackDetInputs'),\n",
      "            ],\n",
      "        ],\n",
      "        type='TestTimeAug'),\n",
      "]\n",
      "val_ann_file = 'annotations/instances_val2017.json'\n",
      "val_batch_size_per_gpu = 1\n",
      "val_cfg = dict(type='ValLoop')\n",
      "val_data_prefix = 'val2017/'\n",
      "val_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        class_text_path='data/texts/lvis_v1_class_texts.json',\n",
      "        dataset=dict(\n",
      "            ann_file='lvis/lvis_v1_minival_inserted_image_name.json',\n",
      "            batch_shapes_cfg=None,\n",
      "            data_prefix=dict(img=''),\n",
      "            data_root='data/coco/',\n",
      "            test_mode=True,\n",
      "            type='YOLOv5LVISV1Dataset'),\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(scale=(\n",
      "                1280,\n",
      "                1280,\n",
      "            ), type='YOLOv5KeepRatioResize'),\n",
      "            dict(\n",
      "                allow_scale_up=False,\n",
      "                pad_val=dict(img=114),\n",
      "                scale=(\n",
      "                    1280,\n",
      "                    1280,\n",
      "                ),\n",
      "                type='LetterResize'),\n",
      "            dict(_scope_='mmdet', type='LoadAnnotations', with_bbox=True),\n",
      "            dict(type='LoadText'),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'scale_factor',\n",
      "                    'pad_param',\n",
      "                    'texts',\n",
      "                ),\n",
      "                type='mmdet.PackDetInputs'),\n",
      "        ],\n",
      "        type='MultiModalDataset'),\n",
      "    drop_last=False,\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    pin_memory=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "val_evaluator = dict(\n",
      "    ann_file='data/coco/lvis/lvis_v1_minival_inserted_image_name.json',\n",
      "    metric='bbox',\n",
      "    proposal_nums=(\n",
      "        100,\n",
      "        1,\n",
      "        10,\n",
      "    ),\n",
      "    type='mmdet.LVISMetric')\n",
      "val_interval_stage2 = 1\n",
      "val_num_workers = 2\n",
      "vis_backends = [\n",
      "    dict(type='LocalVisBackend'),\n",
      "]\n",
      "visualizer = dict(\n",
      "    name='visualizer',\n",
      "    type='mmdet.DetLocalVisualizer',\n",
      "    vis_backends=[\n",
      "        dict(type='LocalVisBackend'),\n",
      "    ])\n",
      "weight_decay = 0.025\n",
      "widen_factor = 1.25\n",
      "work_dir = '.'\n",
      "\n",
      "12/23 19:39:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "12/23 19:39:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(49          ) EMAHook                            \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_load_checkpoint:\n",
      "(49          ) EMAHook                            \n",
      " -------------------- \n",
      "before_train:\n",
      "(9           ) YOLOv5ParamSchedulerHook           \n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(49          ) EMAHook                            \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      "(NORMAL      ) PipelineSwitchHook                 \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(9           ) YOLOv5ParamSchedulerHook           \n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(9           ) YOLOv5ParamSchedulerHook           \n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(49          ) EMAHook                            \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(9           ) YOLOv5ParamSchedulerHook           \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(49          ) EMAHook                            \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DetVisualizationHook               \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(9           ) YOLOv5ParamSchedulerHook           \n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(49          ) EMAHook                            \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_save_checkpoint:\n",
      "(49          ) EMAHook                            \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(49          ) EMAHook                            \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DetVisualizationHook               \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(49          ) EMAHook                            \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "Loads checkpoint by local backend from path: yolo_world_v2_x_obj365v1_goldg_cc3mlite_pretrain_1280ft-14996a36.pth\n",
      "12/23 19:39:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Load checkpoint from yolo_world_v2_x_obj365v1_goldg_cc3mlite_pretrain_1280ft-14996a36.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "YOLOWorldDetector(\n",
       "  (data_preprocessor): YOLOWDetDataPreprocessor()\n",
       "  (backbone): MultiModalYOLOBackbone(\n",
       "    (image_model): YOLOv8CSPDarknet(\n",
       "      (stem): ConvModule(\n",
       "        (conv): Conv2d(3, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (activate): SiLU(inplace=True)\n",
       "      )\n",
       "      (stage1): Sequential(\n",
       "        (0): ConvModule(\n",
       "          (conv): Conv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (activate): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): CSPLayerWithTwoConv(\n",
       "          (main_conv): ConvModule(\n",
       "            (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (final_conv): ConvModule(\n",
       "            (conv): Conv2d(400, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (blocks): ModuleList(\n",
       "            (0): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (stage2): Sequential(\n",
       "        (0): ConvModule(\n",
       "          (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (activate): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): CSPLayerWithTwoConv(\n",
       "          (main_conv): ConvModule(\n",
       "            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (final_conv): ConvModule(\n",
       "            (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (blocks): ModuleList(\n",
       "            (0): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (3): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (4): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (5): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (stage3): Sequential(\n",
       "        (0): ConvModule(\n",
       "          (conv): Conv2d(320, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (activate): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): CSPLayerWithTwoConv(\n",
       "          (main_conv): ConvModule(\n",
       "            (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (final_conv): ConvModule(\n",
       "            (conv): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (blocks): ModuleList(\n",
       "            (0): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (3): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (4): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (5): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (stage4): Sequential(\n",
       "        (0): ConvModule(\n",
       "          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (activate): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): CSPLayerWithTwoConv(\n",
       "          (main_conv): ConvModule(\n",
       "            (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (final_conv): ConvModule(\n",
       "            (conv): Conv2d(1600, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (blocks): ModuleList(\n",
       "            (0): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): DarknetBottleneck(\n",
       "              (conv1): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "              (conv2): ConvModule(\n",
       "                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (activate): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): SPPFBottleneck(\n",
       "          (conv1): ConvModule(\n",
       "            (conv): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (poolings): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "          (conv2): ConvModule(\n",
       "            (conv): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (text_model): HuggingCLIPLanguageBackbone(\n",
       "      (model): CLIPTextModelWithProjection(\n",
       "        (text_model): CLIPTextTransformer(\n",
       "          (embeddings): CLIPTextEmbeddings(\n",
       "            (token_embedding): Embedding(49408, 512)\n",
       "            (position_embedding): Embedding(77, 512)\n",
       "          )\n",
       "          (encoder): CLIPEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (1): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (2): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (3): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (4): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (5): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (6): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (7): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (8): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (9): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (10): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (11): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (neck): YOLOWorldPAFPN(\n",
       "    (reduce_layers): ModuleList(\n",
       "      (0): Identity()\n",
       "      (1): Identity()\n",
       "      (2): Identity()\n",
       "    )\n",
       "    (upsample_layers): ModuleList(\n",
       "      (0): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (1): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    )\n",
       "    (top_down_layers): ModuleList(\n",
       "      (0): MaxSigmoidCSPLayerWithTwoConv(\n",
       "        (main_conv): ConvModule(\n",
       "          (conv): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (activate): SiLU(inplace=True)\n",
       "        )\n",
       "        (final_conv): ConvModule(\n",
       "          (conv): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (activate): SiLU(inplace=True)\n",
       "        )\n",
       "        (blocks): ModuleList(\n",
       "          (0): DarknetBottleneck(\n",
       "            (conv1): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "            (conv2): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (1): DarknetBottleneck(\n",
       "            (conv1): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "            (conv2): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (2): DarknetBottleneck(\n",
       "            (conv1): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "            (conv2): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (attn_block): MaxSigmoidAttnBlock(\n",
       "          (guide_fc): Linear(in_features=512, out_features=320, bias=True)\n",
       "          (project_conv): ConvModule(\n",
       "            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): MaxSigmoidCSPLayerWithTwoConv(\n",
       "        (main_conv): ConvModule(\n",
       "          (conv): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (activate): SiLU(inplace=True)\n",
       "        )\n",
       "        (final_conv): ConvModule(\n",
       "          (conv): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (activate): SiLU(inplace=True)\n",
       "        )\n",
       "        (blocks): ModuleList(\n",
       "          (0): DarknetBottleneck(\n",
       "            (conv1): ConvModule(\n",
       "              (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "            (conv2): ConvModule(\n",
       "              (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (1): DarknetBottleneck(\n",
       "            (conv1): ConvModule(\n",
       "              (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "            (conv2): ConvModule(\n",
       "              (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (2): DarknetBottleneck(\n",
       "            (conv1): ConvModule(\n",
       "              (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "            (conv2): ConvModule(\n",
       "              (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (attn_block): MaxSigmoidAttnBlock(\n",
       "          (guide_fc): Linear(in_features=512, out_features=160, bias=True)\n",
       "          (project_conv): ConvModule(\n",
       "            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (downsample_layers): ModuleList(\n",
       "      (0): ConvModule(\n",
       "        (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (activate): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): ConvModule(\n",
       "        (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (activate): SiLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (bottom_up_layers): ModuleList(\n",
       "      (0): MaxSigmoidCSPLayerWithTwoConv(\n",
       "        (main_conv): ConvModule(\n",
       "          (conv): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (activate): SiLU(inplace=True)\n",
       "        )\n",
       "        (final_conv): ConvModule(\n",
       "          (conv): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (activate): SiLU(inplace=True)\n",
       "        )\n",
       "        (blocks): ModuleList(\n",
       "          (0): DarknetBottleneck(\n",
       "            (conv1): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "            (conv2): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (1): DarknetBottleneck(\n",
       "            (conv1): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "            (conv2): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (2): DarknetBottleneck(\n",
       "            (conv1): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "            (conv2): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (attn_block): MaxSigmoidAttnBlock(\n",
       "          (guide_fc): Linear(in_features=512, out_features=320, bias=True)\n",
       "          (project_conv): ConvModule(\n",
       "            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): MaxSigmoidCSPLayerWithTwoConv(\n",
       "        (main_conv): ConvModule(\n",
       "          (conv): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (activate): SiLU(inplace=True)\n",
       "        )\n",
       "        (final_conv): ConvModule(\n",
       "          (conv): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (activate): SiLU(inplace=True)\n",
       "        )\n",
       "        (blocks): ModuleList(\n",
       "          (0): DarknetBottleneck(\n",
       "            (conv1): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "            (conv2): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (1): DarknetBottleneck(\n",
       "            (conv1): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "            (conv2): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (2): DarknetBottleneck(\n",
       "            (conv1): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "            (conv2): ConvModule(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (activate): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (attn_block): MaxSigmoidAttnBlock(\n",
       "          (guide_fc): Linear(in_features=512, out_features=320, bias=True)\n",
       "          (project_conv): ConvModule(\n",
       "            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (out_layers): ModuleList(\n",
       "      (0): Identity()\n",
       "      (1): Identity()\n",
       "      (2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (bbox_head): YOLOWorldHead(\n",
       "    (head_module): YOLOWorldHeadModule(\n",
       "      (cls_preds): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvModule(\n",
       "            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(320, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvModule(\n",
       "            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(320, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvModule(\n",
       "            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(320, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (reg_preds): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(320, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvModule(\n",
       "            (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(80, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(640, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvModule(\n",
       "            (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(80, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(640, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvModule(\n",
       "            (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (activate): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(80, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (cls_contrasts): ModuleList(\n",
       "        (0): BNContrastiveHead(\n",
       "          (norm): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BNContrastiveHead(\n",
       "          (norm): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BNContrastiveHead(\n",
       "          (norm): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (loss_cls): CrossEntropyLoss(avg_non_ignore=False)\n",
       "    (loss_bbox): IoULoss()\n",
       "    (loss_obj): None\n",
       "    (assigner): BatchTaskAlignedAssigner()\n",
       "    (loss_dfl): DistributionFocalLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = Config.fromfile(\n",
    "        \"yolo/configs/pretrain/yolo_world_v2_x_vlpan_bn_2e-3_100e_4x8gpus_obj365v1_goldg_train_1280ft_lvis_minival.py\"\n",
    "    )\n",
    "cfg.work_dir = \".\"\n",
    "cfg.load_from = \"yolo_world_v2_x_obj365v1_goldg_cc3mlite_pretrain_1280ft-14996a36.pth\"\n",
    "runner = Runner.from_cfg(cfg)\n",
    "runner.call_hook(\"before_run\")\n",
    "runner.load_or_resume()\n",
    "pipeline = cfg.test_dataloader.dataset.pipeline\n",
    "runner.pipeline = Compose(pipeline)\n",
    "runner.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4b44b76-5610-4f9e-8a84-8fbc57035bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_detector(model, image, texts, test_pipeline, score_thr=0.3):\n",
    "    data_info = dict(img_id=0, img=image, texts=texts)\n",
    "    data_info = test_pipeline(data_info)\n",
    "    data_batch = dict(inputs=data_info['inputs'].unsqueeze(0),\n",
    "                      data_samples=[data_info['data_samples']])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.test_step(data_batch)[0]\n",
    "        pred_instances = output.pred_instances\n",
    "        pred_instances = pred_instances[pred_instances.scores.float() >\n",
    "                                        score_thr]\n",
    "    output.pred_instances = pred_instances\n",
    "    return output\n",
    "\n",
    "def run_yoloworld(video_path, text_query):\n",
    "    model = init_detector(\n",
    "        \"yolo/configs/pretrain/yolo_world_v2_x_vlpan_bn_2e-3_100e_4x8gpus_obj365v1_goldg_train_1280ft_lvis_minival.py\",\n",
    "        \"yolo_world_v2_x_obj365v1_goldg_cc3mlite_pretrain_1280ft-14996a36.pth\",\n",
    "        device=\"cuda:0\"\n",
    "    )\n",
    "    model.cfg.test_dataloader.dataset.pipeline[0].type='mmdet.LoadImageFromNDArray'\n",
    "    test_pipeline = Compose(model.cfg.test_dataloader.dataset.pipeline)\n",
    "    model.reparameterize([text_query])\n",
    "    video_reader = mmcv.VideoReader(video_path)\n",
    "    frame_count = 0\n",
    "    results = {}\n",
    "    for frame in track_iter_progress(video_reader):\n",
    "        result = inference_detector(model,\n",
    "                                    frame,\n",
    "                                    [text_query],\n",
    "                                    test_pipeline,\n",
    "                                    score_thr=0.3)\n",
    "        results[frame_count] = result\n",
    "    \n",
    "    # cap = cv2.VideoCapture(video_path)\n",
    "    # while cap.isOpened():\n",
    "    #     ret, frame = cap.read()\n",
    "    #     if not ret:\n",
    "    #         break\n",
    "    #     frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    #     # frame = torch.from_numpy(frame).float().to(\"cuda\")\n",
    "    #     frame = Image.fromarray(frame)\n",
    "    #     data_info = runner.pipeline(dict(img_id=0, img_path=frame, texts=[text_query]))\n",
    "    #     frame_count += 1\n",
    "    # cap.release()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6632b96f-3a6c-4f32-9a67-dd713ff81b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: yolo_world_v2_x_obj365v1_goldg_cc3mlite_pretrain_1280ft-14996a36.pth\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/coco/lvis/lvis_v1_minival_inserted_image_name.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_yoloworld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhong_kong_airport_demo_data.mp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMcDonalds Logo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m, in \u001b[0;36mrun_yoloworld\u001b[0;34m(video_path, text_query)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_yoloworld\u001b[39m(video_path, text_query):\n\u001b[0;32m---> 16\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43minit_detector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myolo/configs/pretrain/yolo_world_v2_x_vlpan_bn_2e-3_100e_4x8gpus_obj365v1_goldg_train_1280ft_lvis_minival.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myolo_world_v2_x_obj365v1_goldg_cc3mlite_pretrain_1280ft-14996a36.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     model\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mtest_dataloader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mpipeline[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmmdet.LoadImageFromNDArray\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     22\u001b[0m     test_pipeline \u001b[38;5;241m=\u001b[39m Compose(model\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mtest_dataloader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mpipeline)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/mmdet/apis/inference.py:97\u001b[0m, in \u001b[0;36minit_detector\u001b[0;34m(config, checkpoint, palette, device, cfg_options)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# lazy init. We only need the metainfo.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m test_dataset_cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlazy_init\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m metainfo \u001b[38;5;241m=\u001b[39m \u001b[43mDATASETS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset_cfg\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmetainfo\n\u001b[1;32m     98\u001b[0m cfg_palette \u001b[38;5;241m=\u001b[39m metainfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpalette\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg_palette \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/mmengine/registry/registry.py:570\u001b[0m, in \u001b[0;36mRegistry.build\u001b[0;34m(self, cfg, *args, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg: \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    549\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build an instance.\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \n\u001b[1;32m    551\u001b[0m \u001b[38;5;124;03m    Build an instance by calling :attr:`build_func`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03m        >>> model = MODELS.build(cfg)\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregistry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/mmengine/registry/build_functions.py:121\u001b[0m, in \u001b[0;36mbuild_from_cfg\u001b[0;34m(cfg, registry, default_args)\u001b[0m\n\u001b[1;32m    119\u001b[0m     obj \u001b[38;5;241m=\u001b[39m obj_cls\u001b[38;5;241m.\u001b[39mget_instance(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 121\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (inspect\u001b[38;5;241m.\u001b[39misclass(obj_cls) \u001b[38;5;129;01mor\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misfunction(obj_cls)\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mismethod(obj_cls)):\n\u001b[1;32m    125\u001b[0m     print_log(\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_cls\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` instance is built from \u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# type: ignore # noqa: E501\u001b[39;00m\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregistry, and its implementation can be found in \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_cls\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    129\u001b[0m         logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurrent\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    130\u001b[0m         level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mDEBUG)\n",
      "File \u001b[0;32m/app/yolo/yolo_world/datasets/mm_dataset.py:25\u001b[0m, in \u001b[0;36mMultiModalDataset.__init__\u001b[0;34m(self, dataset, class_text_path, test_mode, pipeline, lazy_init)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset: BaseDataset\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m \u001b[43mDATASETS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, BaseDataset):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m dataset\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/mmengine/registry/registry.py:570\u001b[0m, in \u001b[0;36mRegistry.build\u001b[0;34m(self, cfg, *args, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg: \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    549\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build an instance.\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \n\u001b[1;32m    551\u001b[0m \u001b[38;5;124;03m    Build an instance by calling :attr:`build_func`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03m        >>> model = MODELS.build(cfg)\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregistry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/mmengine/registry/build_functions.py:121\u001b[0m, in \u001b[0;36mbuild_from_cfg\u001b[0;34m(cfg, registry, default_args)\u001b[0m\n\u001b[1;32m    119\u001b[0m     obj \u001b[38;5;241m=\u001b[39m obj_cls\u001b[38;5;241m.\u001b[39mget_instance(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 121\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (inspect\u001b[38;5;241m.\u001b[39misclass(obj_cls) \u001b[38;5;129;01mor\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misfunction(obj_cls)\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mismethod(obj_cls)):\n\u001b[1;32m    125\u001b[0m     print_log(\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_cls\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` instance is built from \u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# type: ignore # noqa: E501\u001b[39;00m\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregistry, and its implementation can be found in \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_cls\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    129\u001b[0m         logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurrent\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    130\u001b[0m         level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mDEBUG)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/mmyolo/datasets/yolov5_coco.py:19\u001b[0m, in \u001b[0;36mBatchShapePolicyDataset.__init__\u001b[0;34m(self, batch_shapes_cfg, *args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     15\u001b[0m              \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m     16\u001b[0m              batch_shapes_cfg: Optional[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_shapes_cfg \u001b[38;5;241m=\u001b[39m batch_shapes_cfg\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/mmdet/datasets/base_det_dataset.py:40\u001b[0m, in \u001b[0;36mBaseDetDataset.__init__\u001b[0;34m(self, seg_map_suffix, proposal_file, file_client_args, backend_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_client_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe `file_client_args` is deprecated, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use `backend_args` instead, please refer to\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://github.com/open-mmlab/mmdetection/blob/main/configs/_base_/datasets/coco_detection.py\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     )\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/mmengine/dataset/base_dataset.py:247\u001b[0m, in \u001b[0;36mBaseDataset.__init__\u001b[0;34m(self, ann_file, metainfo, data_root, data_prefix, filter_cfg, indices, serialize_data, pipeline, test_mode, lazy_init, max_refetch)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# Full initialize the dataset.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lazy_init:\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/mmyolo/datasets/yolov5_coco.py:27\u001b[0m, in \u001b[0;36mBatchShapePolicyDataset.full_init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# load data information\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# batch_shapes_cfg\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_shapes_cfg:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/mmdet/datasets/lvis.py:605\u001b[0m, in \u001b[0;36mLVISV1Dataset.load_data_list\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    601\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPackage lvis is not installed. Please run \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpip install git+https://github.com/lvis-dataset/lvis-api.git\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m    602\u001b[0m     )\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_local_path(\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mann_file, backend_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend_args) \u001b[38;5;28;01mas\u001b[39;00m local_path:\n\u001b[0;32m--> 605\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlvis \u001b[38;5;241m=\u001b[39m \u001b[43mLVIS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcat_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlvis\u001b[38;5;241m.\u001b[39mget_cat_ids()\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcat2label \u001b[38;5;241m=\u001b[39m {cat_id: i \u001b[38;5;28;01mfor\u001b[39;00m i, cat_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcat_ids)}\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lvis/lvis.py:27\u001b[0m, in \u001b[0;36mLVIS.__init__\u001b[0;34m(self, annotation_path)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading annotations.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotation_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mdict\u001b[39m\n\u001b[1;32m     31\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnnotation file format \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_index()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lvis/lvis.py:35\u001b[0m, in \u001b[0;36mLVIS._load_json\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/coco/lvis/lvis_v1_minival_inserted_image_name.json'"
     ]
    }
   ],
   "source": [
    "run_yoloworld('hong_kong_airport_demo_data.mp4', 'McDonalds Logo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f973dde0-b8c3-449e-8985-e4a1cbc96b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: BoxAnnotator is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
     ]
    }
   ],
   "source": [
    "import PIL.Image\n",
    "import supervision as sv\n",
    "\n",
    "bounding_box_annotator = sv.BoxAnnotator()\n",
    "label_annotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\n",
    "\n",
    "class_names = (\"person, bicycle, car, motorcycle, airplane, bus, train, truck, boat, \"\n",
    "               \"traffic light, fire hydrant, stop sign, parking meter, bench, bird, \"\n",
    "               \"cat, dog, horse, sheep, cow, elephant, bear, zebra, giraffe, \"\n",
    "               \"backpack, umbrella, handbag, tie, suitcase, frisbee, skis, snowboard, \"\n",
    "               \"sports ball, kite, baseball bat, baseball glove, skateboard, \"\n",
    "               \"surfboard, tennis racket, bottle, wine glass, cup, fork, knife, \"\n",
    "               \"spoon, bowl, banana, apple, sandwich, orange, broccoli, carrot, \"\n",
    "               \"hot dog, pizza, donut, cake, chair, couch, potted plant, bed, \"\n",
    "               \"dining table, toilet, tv, laptop, mouse, remote, keyboard, \"\n",
    "               \"cell phone, microwave, oven, toaster, sink, refrigerator, book, \"\n",
    "               \"clock, vase, scissors, teddy bear, hair drier, toothbrush\")\n",
    "\n",
    "class_names2 = (\"dog, eye, tongue, ear, leash\")\n",
    "\n",
    "def run_image(\n",
    "        runner,\n",
    "        input_image,\n",
    "        max_num_boxes=100,\n",
    "        score_thr=0.05,\n",
    "        nms_thr=0.5,\n",
    "        output_image=\"output.png\",\n",
    "):\n",
    "    texts = [[t.strip()] for t in class_names.split(\",\")] + [[\" \"]]\n",
    "    data_info = runner.pipeline(dict(img_id=0, img_path=input_image,\n",
    "                                     texts=texts))\n",
    "\n",
    "    data_batch = dict(\n",
    "        inputs=data_info[\"inputs\"].unsqueeze(0),\n",
    "        data_samples=[data_info[\"data_samples\"]],\n",
    "    )\n",
    "\n",
    "    with autocast(enabled=False), torch.no_grad():\n",
    "        output = runner.model.test_step(data_batch)[0]\n",
    "        runner.model.class_names = texts\n",
    "        pred_instances = output.pred_instances\n",
    "\n",
    "    keep_idxs = nms(pred_instances.bboxes, pred_instances.scores, iou_threshold=nms_thr)\n",
    "    pred_instances = pred_instances[keep_idxs]\n",
    "    pred_instances = pred_instances[pred_instances.scores.float() > score_thr]\n",
    "\n",
    "    if len(pred_instances.scores) > max_num_boxes:\n",
    "        indices = pred_instances.scores.float().topk(max_num_boxes)[1]\n",
    "        pred_instances = pred_instances[indices]\n",
    "    output.pred_instances = pred_instances\n",
    "\n",
    "    pred_instances = pred_instances.cpu().numpy()\n",
    "    detections = sv.Detections(\n",
    "        xyxy=pred_instances['bboxes'],\n",
    "        class_id=pred_instances['labels'],\n",
    "        confidence=pred_instances['scores']\n",
    "    )\n",
    "\n",
    "    labels = [\n",
    "        f\"{class_id} {confidence:0.2f}\"\n",
    "        for class_id, confidence\n",
    "        in zip(detections.class_id, detections.confidence)\n",
    "    ]\n",
    "\n",
    "    image = PIL.Image.open(input_image)\n",
    "    svimage = np.array(image)\n",
    "    svimage = bounding_box_annotator.annotate(svimage, detections)\n",
    "    svimage = label_annotator.annotate(svimage, detections, labels)\n",
    "    return svimage[:, :, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e41e3ab2-0a1f-43d4-81e8-a547997f38e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "nms_impl: implementation for device cuda:0 not found.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mrun_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdog.jpeg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m sv\u001b[38;5;241m.\u001b[39mplot_image(img)\n",
      "Cell \u001b[0;32mIn[7], line 39\u001b[0m, in \u001b[0;36mrun_image\u001b[0;34m(runner, input_image, max_num_boxes, score_thr, nms_thr, output_image)\u001b[0m\n\u001b[1;32m     33\u001b[0m data_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     34\u001b[0m     inputs\u001b[38;5;241m=\u001b[39mdata_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     35\u001b[0m     data_samples\u001b[38;5;241m=\u001b[39m[data_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 39\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     40\u001b[0m     runner\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mclass_names \u001b[38;5;241m=\u001b[39m texts\n\u001b[1;32m     41\u001b[0m     pred_instances \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mpred_instances\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/mmengine/model/base_model/base_model.py:145\u001b[0m, in \u001b[0;36mBaseModel.test_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"``BaseModel`` implements ``test_step`` the same as ``val_step``.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m    list: The predictions of given data.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    144\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_preprocessor(data, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/mmengine/model/base_model/base_model.py:361\u001b[0m, in \u001b[0;36mBaseModel._run_forward\u001b[0;34m(self, data, mode)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Unpacks data for :meth:`forward`\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;03m    dict or list: Results of training or testing mode.\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 361\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    363\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39mdata, mode\u001b[38;5;241m=\u001b[39mmode)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/mmdet/models/detectors/base.py:94\u001b[0m, in \u001b[0;36mBaseDetector.forward\u001b[0;34m(self, inputs, data_samples, mode)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(inputs, data_samples)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensor\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(inputs, data_samples)\n",
      "File \u001b[0;32m/app/yolo/yolo_world/models/detectors/yolo_world.py:48\u001b[0m, in \u001b[0;36mYOLOWorldDetector.predict\u001b[0;34m(self, batch_inputs, batch_data_samples, rescale)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# self.bbox_head.num_classes = self.num_test_classes\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbbox_head\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m txt_feats[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 48\u001b[0m results_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbbox_head\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_feats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mtxt_feats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mbatch_data_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mrescale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrescale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m batch_data_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_pred_to_datasample(\n\u001b[1;32m     54\u001b[0m     batch_data_samples, results_list)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch_data_samples\n",
      "File \u001b[0;32m/app/yolo/yolo_world/models/dense_heads/yolo_world_head.py:408\u001b[0m, in \u001b[0;36mYOLOWorldHead.predict\u001b[0;34m(self, img_feats, txt_feats, batch_data_samples, rescale)\u001b[0m\n\u001b[1;32m    404\u001b[0m batch_img_metas \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    405\u001b[0m     data_samples\u001b[38;5;241m.\u001b[39mmetainfo \u001b[38;5;28;01mfor\u001b[39;00m data_samples \u001b[38;5;129;01min\u001b[39;00m batch_data_samples\n\u001b[1;32m    406\u001b[0m ]\n\u001b[1;32m    407\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(img_feats, txt_feats)\n\u001b[0;32m--> 408\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_by_feat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mouts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mbatch_img_metas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_img_metas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mrescale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrescale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[0;32m/app/yolo/yolo_world/models/dense_heads/yolo_world_head.py:725\u001b[0m, in \u001b[0;36mYOLOWorldHead.predict_by_feat\u001b[0;34m(self, cls_scores, bbox_preds, objectnesses, batch_img_metas, cfg, rescale, with_nms)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolox_style\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;66;03m# do not need max_per_img\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     cfg\u001b[38;5;241m.\u001b[39mmax_per_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(results)\n\u001b[0;32m--> 725\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bbox_post_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mrescale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mwith_nms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_nms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mimg_meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_meta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m results\u001b[38;5;241m.\u001b[39mbboxes[:, \u001b[38;5;241m0\u001b[39m::\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mclamp_(\u001b[38;5;241m0\u001b[39m, ori_shape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    731\u001b[0m results\u001b[38;5;241m.\u001b[39mbboxes[:, \u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mclamp_(\u001b[38;5;241m0\u001b[39m, ori_shape[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/mmdet/models/dense_heads/base_dense_head.py:479\u001b[0m, in \u001b[0;36mBaseDenseHead._bbox_post_process\u001b[0;34m(self, results, cfg, rescale, with_nms, img_meta)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_nms \u001b[38;5;129;01mand\u001b[39;00m results\u001b[38;5;241m.\u001b[39mbboxes\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    478\u001b[0m     bboxes \u001b[38;5;241m=\u001b[39m get_box_tensor(results\u001b[38;5;241m.\u001b[39mbboxes)\n\u001b[0;32m--> 479\u001b[0m     det_bboxes, keep_idxs \u001b[38;5;241m=\u001b[39m \u001b[43mbatched_nms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m     results \u001b[38;5;241m=\u001b[39m results[keep_idxs]\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;66;03m# some nms would reweight the score, such as softnms\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/mmcv/ops/nms.py:302\u001b[0m, in \u001b[0;36mbatched_nms\u001b[0;34m(boxes, scores, idxs, nms_cfg, class_agnostic)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# Won't split to multiple nms nodes when exporting to onnx\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m boxes_for_nms\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m split_thr:\n\u001b[0;32m--> 302\u001b[0m     dets, keep \u001b[38;5;241m=\u001b[39m \u001b[43mnms_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboxes_for_nms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnms_cfg_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m boxes[keep]\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;66;03m# This assumes `dets` has arbitrary dimensions where\u001b[39;00m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;66;03m# the last dimension is score.\u001b[39;00m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;66;03m# Currently it supports bounding boxes [x1, y1, x2, y2, score] or\u001b[39;00m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;66;03m# rotated boxes [cx, cy, w, h, angle_radian, score].\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/mmengine/utils/misc.py:395\u001b[0m, in \u001b[0;36mdeprecated_api_warning.<locals>.api_warning_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m             kwargs[dst_arg_name] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(src_arg_name)\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# apply converted arguments to the decorated method\u001b[39;00m\n\u001b[0;32m--> 395\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mold_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/mmcv/ops/nms.py:127\u001b[0m, in \u001b[0;36mnms\u001b[0;34m(boxes, scores, iou_threshold, offset, score_threshold, max_num)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m boxes\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m scores\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m offset \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 127\u001b[0m inds \u001b[38;5;241m=\u001b[39m \u001b[43mNMSop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mmax_num\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m dets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((boxes[inds], scores[inds]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_numpy:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/mmcv/ops/nms.py:27\u001b[0m, in \u001b[0;36mNMSop.forward\u001b[0;34m(ctx, bboxes, scores, iou_threshold, offset, score_threshold, max_num)\u001b[0m\n\u001b[1;32m     23\u001b[0m     bboxes, scores \u001b[38;5;241m=\u001b[39m bboxes[valid_mask], scores[valid_mask]\n\u001b[1;32m     24\u001b[0m     valid_inds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnonzero(\n\u001b[1;32m     25\u001b[0m         valid_mask, as_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39msqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m inds \u001b[38;5;241m=\u001b[39m \u001b[43mext_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnms\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_num \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     31\u001b[0m     inds \u001b[38;5;241m=\u001b[39m inds[:max_num]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: nms_impl: implementation for device cuda:0 not found.\n"
     ]
    }
   ],
   "source": [
    "img = run_image(runner,\"dog.jpeg\")\n",
    "sv.plot_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1e83aa-44e9-43ff-8990-f4c1ceba3005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
